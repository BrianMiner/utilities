{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jma/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/jma/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "PATH = \"/home/jma/Desktop/march_embedding/\"\n",
    "\n",
    "import keras\n",
    "from keras import *\n",
    "from keras.layers import Embedding, Input,concatenate, Flatten,dot, Dense,LSTM\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>I want to look at how a batch of reviews look in and out of Embedding layer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [223, 136, 15, 256, 14, 65, 318, 82, 5, 182, 3...\n",
       "1    [164, 359, 15, 256, 14, 6, 77, 157, 82, 57, 22...\n",
       "2    [22, 175, 14, 151, 218, 57, 315, 73, 341, 271,...\n",
       "3    [261, 22, 72, 129, 258, 219, 248, 234, 45, 28,...\n",
       "4    [181, 15, 103, 82, 5, 57, 272, 247, 97, 22, 35...\n",
       "Name: fullreview, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load reviews and limit to 10\n",
    "train=pd.read_csv('/media/jma/DATA/amazon_review_full_csv/train.csv',header=0,nrows=10, names=['rating','short','fullreview'])\n",
    "train['fullreview']=train['fullreview'].replace(regex=True,to_replace='[^\\sa-zA-Z]', value=r'')\n",
    "X=train.fullreview.str.lower()\n",
    "X=X.str.split() #now each instance is a list of tokens \n",
    "\n",
    "\n",
    "#loop through each row and create a dictionary of words\n",
    "\n",
    "\n",
    "dct_word_freq={}\n",
    "\n",
    "for x in X.iteritems():\n",
    "    for v in x[1]:\n",
    "        if v in dct_word_freq:\n",
    "            dct_word_freq[v]+=1\n",
    "        else:\n",
    "            dct_word_freq[v]=1\n",
    "            \n",
    "\n",
    "#now, could remove those with low number of instances (or set them to UNK)\n",
    "#TODO\n",
    "\n",
    "\n",
    "\n",
    "#now create a dictionary where each unique word is mapped to an int (add 2 so we dont have 0 (padding) or 1 (unk) used)\n",
    "dct_word_to_int={o:i+2 for i,o in enumerate(set(dct_word_freq.keys()))}\n",
    "dct_word_to_int['unk']=1\n",
    "\n",
    "print(dct_word_to_int['the'])#mapped to 71\n",
    "print(dct_word_to_int['unk'])#mapped to 1\n",
    "\n",
    "\n",
    "#now replace each word in the list of tokens in X with its corresponding int\n",
    "#would so same exact thing for test sets\n",
    "\n",
    "def replace_to_int(x,dct_word_to_int):\n",
    "    hold_ints=[]\n",
    "    for token in x:\n",
    "        if token in dct_word_to_int:\n",
    "            hold_ints.append(dct_word_to_int[token])\n",
    "        else:\n",
    "            hold_ints.append(1) #unknown\n",
    "    return(hold_ints)\n",
    "    \n",
    "z=X.apply(replace_to_int,args=(dct_word_to_int,))\n",
    "z.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 50)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now can use keras padding\n",
    "\n",
    "maxlen=50\n",
    "\n",
    "#shape is now (batche size, # time steps)\n",
    "z=keras.preprocessing.sequence.pad_sequences(z.values,maxlen=maxlen)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 50, 10)            3600      \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               71168     \n",
      "=================================================================\n",
      "Total params: 74,768\n",
      "Trainable params: 74,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(dct_word_to_int)\n",
    "embedding_size=10\n",
    "\n",
    "embedding_layer = Embedding(output_dim=embedding_size, input_dim=vocab_size,input_length=maxlen, mask_zero=True)\n",
    "\n",
    "x = Input(shape=[maxlen])\n",
    "embedding = embedding_layer(x)\n",
    "out=LSTM(128, dropout=0.2, recurrent_dropout=0.2)(embedding)\n",
    "\n",
    "model = Model(inputs=x, outputs=out)\n",
    "\n",
    "#so out (and thus into LSTM) of the embedding is size (batch size, # time steps (aka sequence length), embedding size  )\n",
    "#size out of the LSTM is (batches and dimension of the LSTM)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
