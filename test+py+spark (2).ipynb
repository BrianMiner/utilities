{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named projectnb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-30462f91923b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @hidden_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprojectnb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProjectContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProjectUtil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProjectContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProjectContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'48e1679f-100b-456b-87ab-0432209e9096'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'p-bed11d2793c7698686080191847c4f0cd0170229'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named projectnb"
     ]
    }
   ],
   "source": [
    "# @hidden_cell\n",
    "# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n",
    "from projectnb import ProjectContext, ProjectUtil\n",
    "pc = ProjectContext.ProjectContext(sc, '48e1679f-100b-456b-87ab-0432209e9096', 'p-bed11d2793c7698686080191847c4f0cd0170229')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.eventLog.enabled', u'true'),\n",
       " (u'spark.driver.extraJavaOptions',\n",
       "  u'-Djavax.net.ssl.trustStore=/user-home/_global_/security/customer-truststores/cacerts'),\n",
       " (u'spark.app.name', u'my-notebook'),\n",
       " (u'spark.driver.host', u'172.16.112.61'),\n",
       " (u'spark.shuffle.service.enabled', u'true'),\n",
       " (u'spark.dynamicAllocation.executorIdleTimeout', u'300'),\n",
       " (u'spark.driver.port', u'39884'),\n",
       " (u'spark.driver.allowMultipleContexts', u'true'),\n",
       " (u'spark.executor.extraJavaOptions',\n",
       "  u'-Djavax.net.ssl.trustStore=/user-home/_global_/security/customer-truststores/cacerts'),\n",
       " (u'spark.executor.id', u'driver'),\n",
       " (u'spark.eventLog.dir', u'/tmp/spark-events'),\n",
       " (u'spark.executor.memory', u'16g'),\n",
       " (u'spark.app.id', u'app-20180318013254-0361'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.ui.enabled', u'false'),\n",
       " (u'spark.sql.warehouse.dir', u'/tmp'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.cores.max', u'8'),\n",
       " (u'spark.dynamicAllocation.initialExecutors', u'1'),\n",
       " (u'spark.master', u'spark://spark-master-svc.ibm-private-cloud:7077'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.dynamicAllocation.enabled', u'true'),\n",
       " (u'spark.port.maxRetries', u'100')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.set('spark.executor.memory', '16g')\n",
    "sc._conf.set('spark.cores.max', '8')\n",
    "sc._conf.set('spark.driver.allowMultipleContexts', 'true')\n",
    "sc._conf.getAll()\n",
    "\n",
    "#write to CSV\n",
    "#data.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").mode(\"overwrite\").save('/user-home/1014/branch_opp_data/binned_exportUS.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark as spark\n",
    "from pyspark.sql import SparkSession, functions as F, Row, Column\n",
    "from pyspark.sql.functions import datediff, unix_timestamp, from_unixtime\n",
    "\n",
    "from pyspark.sql.types import DateType, StringType, StructField, IntegerType, FloatType, StructType\n",
    "from datetime import date\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#https://stackoverflow.com/questions/36584812/pyspark-row-wise-function-composition\n",
    "#https://stackoverflow.com/questions/40389433/pyspark-dataframe-apply-function-to-two-columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Load DataFrames from DSX</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+\n",
      "|  ID|LMI_HMT|SQFT|\n",
      "+----+-------+----+\n",
      "| 152|   null|4459|\n",
      "| 145|   null|3000|\n",
      "| 141|   null|3240|\n",
      "| 143|   null|3400|\n",
      "|2917|    LMI| 540|\n",
      "| 117|    LMI|3000|\n",
      "| 105|   null|3900|\n",
      "| 142|   null|3000|\n",
      "| 683|    LMI|2815|\n",
      "| 725|   null|3277|\n",
      "| 168|    LMI|1155|\n",
      "| 208|   null|2500|\n",
      "| 157|   null|6085|\n",
      "| 209|   null|5832|\n",
      "| 721| LMIHMT|3211|\n",
      "| 549|   null|4155|\n",
      "| 719| LMIHMT|4371|\n",
      "|2924|   null| 555|\n",
      "|6390|   null|4200|\n",
      "|1747|   null|4200|\n",
      "+----+-------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+--------------+--------------------+\n",
      "|   ID|UrbanicityCode|      UrbanicityName|\n",
      "+-----+--------------+--------------------+\n",
      "| 3000|             6|               Rural|\n",
      "|43328|             2|     Urban Periphery|\n",
      "|  347|             4|  Suburban Periphery|\n",
      "| 8873|             6|               Rural|\n",
      "| 2900|             2|     Urban Periphery|\n",
      "| 2884|             1|Principal Urban C...|\n",
      "| 1260|             4|  Suburban Periphery|\n",
      "| 2888|             5|          Semirurals|\n",
      "|  343|             4|  Suburban Periphery|\n",
      "| 2863|             4|  Suburban Periphery|\n",
      "|  344|             4|  Suburban Periphery|\n",
      "| 2886|             2|     Urban Periphery|\n",
      "|  346|             4|  Suburban Periphery|\n",
      "| 2867|             3|        Metro Cities|\n",
      "| 5063|             4|  Suburban Periphery|\n",
      "| 2866|             4|  Suburban Periphery|\n",
      "| 2188|             4|  Suburban Periphery|\n",
      "|  353|             4|  Suburban Periphery|\n",
      "|43306|             4|  Suburban Periphery|\n",
      "| 8872|             4|  Suburban Periphery|\n",
      "+-----+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Access 'sq_lmi.txt' data file from the project.\n",
    "sq_lmi = ProjectUtil.load_dataframe_from_file(pc, \"sq_lmi.txt\")\n",
    "sq_lmi.show()\n",
    "# Access 'Branch_Urbanicity.csv' data file from the project.\n",
    "urban= ProjectUtil.load_dataframe_from_file(pc, \"Branch_Urbanicity.csv\")\n",
    "urban.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Selecting, Viewing, Filtering, Aggregating</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- LMI_HMT: string (nullable = true)\n",
      " |-- SQFT: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sq_lmi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|  ID|SQFT|\n",
      "+----+----+\n",
      "| 152|4459|\n",
      "| 145|3000|\n",
      "| 141|3240|\n",
      "| 143|3400|\n",
      "|2917| 540|\n",
      "+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sq_lmi.select('ID','SQFT').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| ID|SQFT|\n",
      "+---+----+\n",
      "|152|4459|\n",
      "|145|3000|\n",
      "|141|3240|\n",
      "|143|3400|\n",
      "|117|3000|\n",
      "+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sq_lmi.select('ID','SQFT').filter(sq_lmi.ID <200).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+\n",
      "| ID|LMI_HMT|SQFT|\n",
      "+---+-------+----+\n",
      "|152|   null|4459|\n",
      "|145|   null|3000|\n",
      "|141|   null|3240|\n",
      "|143|   null|3400|\n",
      "|117|    LMI|3000|\n",
      "+---+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sq_lmi.filter(sq_lmi.ID <200).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+--------+\n",
      "|LMI_HMT|min_sqft|max(SQFT)|count(1)|\n",
      "+-------+--------+---------+--------+\n",
      "|   null|     302|    37536|     797|\n",
      "|    HMT|     522|     6000|      54|\n",
      "|    LMI|     288|    16000|     186|\n",
      "| LMIHMT|     598|    24930|     120|\n",
      "+-------+--------+---------+--------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "agged=sq_lmi.groupby('LMI_HMT').agg(F.min(\"SQFT\").alias(\"min_sqft\"),F.max(\"SQFT\"),F.count('*'))\n",
    "print(agged.show(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "960\n",
      "+----+-------+----+-----+--------------+\n",
      "|  ID|LMI_HMT|SQFT|dummy|UrbanicityCode|\n",
      "+----+-------+----+-----+--------------+\n",
      "| 152|   null|4459| TEST|             4|\n",
      "| 145|   null|3000| TEST|             3|\n",
      "| 141|   null|3240| TEST|             4|\n",
      "| 143|   null|3400| TEST|             1|\n",
      "|2917|    LMI| 540| TEST|             3|\n",
      "+----+-------+----+-----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sql\n",
    "\n",
    "sq_lmi.createOrReplaceTempView(\"tbl_1\")\n",
    "urban.createOrReplaceTempView(\"tbl_2\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sqlDf = spark.sql(\"select a.*, 'TEST' as dummy, b.UrbanicityCode from tbl_1 a inner join tbl_2 b on a.ID=b.ID where a.SQFT <5000\")\n",
    "print(type(sqlDf))\n",
    "print(sqlDf.count())\n",
    "sqlDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+--------+-------------------+\n",
      "|  ID|LMI_HMT|SQFT|SQFT_000|            SQFT_ID|\n",
      "+----+-------+----+--------+-------------------+\n",
      "| 152|   null|4459|   4.459| 29.335526315789473|\n",
      "| 145|   null|3000|     3.0| 20.689655172413794|\n",
      "| 141|   null|3240|    3.24|  22.97872340425532|\n",
      "| 143|   null|3400|     3.4| 23.776223776223777|\n",
      "|2917|    LMI| 540|    0.54|0.18512170037709977|\n",
      "+----+-------+----+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "+----+-------+----+-------------------+\n",
      "|  ID|LMI_HMT|SQFT|            SQFT_ID|\n",
      "+----+-------+----+-------------------+\n",
      "| 152|   null|4459| 29.335526315789473|\n",
      "| 145|   null|3000| 20.689655172413794|\n",
      "| 141|   null|3240|  22.97872340425532|\n",
      "| 143|   null|3400| 23.776223776223777|\n",
      "|2917|    LMI| 540|0.18512170037709977|\n",
      "+----+-------+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#add new column (can chain together using where to filter as you go)\n",
    "sq_lmi=sq_lmi.withColumn('SQFT_000', sq_lmi.SQFT/1000).withColumn('SQFT_ID', sq_lmi.SQFT/sq_lmi.ID)\n",
    "print(sq_lmi.show(5))\n",
    "\n",
    "sq_lmi=sq_lmi.drop('SQFT_000')\n",
    "print(sq_lmi.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Read in data FTP'd as a text file and create data frame</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'Path does not exist: file:/user-home/1014/checking_models/scale.csv;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-eb6ad3f1d016>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"com.databricks.spark.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/user-home/1014/checking_models/scale.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'Path does not exist: file:/user-home/1014/checking_models/scale.csv;'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "scale = (sqlContext.read.format(\"com.databricks.spark.csv\").options(header= \"true\",delimiter=',').load(\"/user-home/1014/checking_models/scale.csv\"))\n",
    "print(type(scale))\n",
    "scale.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Read in with Pandas and transform to a DF</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"/user-home/1019/scale.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Apply a function to a row of a data frame - adding an additional variable</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- LMI_HMT: string (nullable = true)\n",
      " |-- SQFT: integer (nullable = true)\n",
      " |-- SQFT_ID: double (nullable = true)\n",
      "\n",
      "None\n",
      "+------------------+\n",
      "|                V1|\n",
      "+------------------+\n",
      "|130807.11184210525|\n",
      "|62068.965517241384|\n",
      "| 74451.06382978724|\n",
      "| 80839.16083916085|\n",
      "| 99.96571820363387|\n",
      "+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, struct\n",
    "from pyspark.sql.types import IntegerType,DoubleType\n",
    "\n",
    "print (sq_lmi.printSchema())\n",
    "\n",
    "#must check for missing values!\n",
    "\n",
    "\n",
    "\n",
    "def calc_V1(row):\n",
    "    if row is not None and row.SQFT_ID is not None and row.SQFT is not None :\n",
    "        a=row.SQFT_ID*row.SQFT\n",
    "\n",
    "        V1=a\n",
    "        return V1\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "calc_V1_udf = udf(calc_V1, DoubleType())  #register the function and its return type\n",
    "\n",
    "new_df = sq_lmi.withColumn(\"V1\", calc_V1_udf(struct([sq_lmi[x] for x in sq_lmi.columns]))) #apply - the struct is needed to send the entire row\n",
    "new_df.select('V1').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Same as above but without UDF which is faster and you dont need to check for nulls</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_df2=sq_lmi.withColumn(\"V1\", sq_lmi.SQFT_ID*sq_lmi.SQFT ) \n",
    "new_df2.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Use this decorater technique from web site https://p058.github.io/p058.github.io/spark/2017/01/08/spark-dataframes.html</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+-------+-----------+--------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|     ID|Case Number|                Date|               Block|IUCR|Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|    Latitude|    Longitude|            Location|\n",
      "+-------+-----------+--------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|9955810|   HY144797|02/08/2015 11:43:...|   081XX S COLES AVE|1811|   NARCOTICS|POSS: CANNABIS 30...|              STREET|  true|   false| 422|       4|   7|            46|      18|     1198273|     1851626|2015|02/15/2015 12:43:...|41.747693646|-87.549035389|(41.747693646, -8...|\n",
      "|9955861|   HY144838|02/08/2015 11:41:...|    118XX S STATE ST|0486|     BATTERY|DOMESTIC BATTERY ...|           APARTMENT|  true|    true| 522|       5|  34|            53|     08B|     1178335|     1826581|2015|02/15/2015 12:43:...|41.679442289|-87.622850758|(41.679442289, -8...|\n",
      "|9955801|   HY144779|02/08/2015 11:30:...| 002XX S LARAMIE AVE|2026|   NARCOTICS|           POSS: PCP|            SIDEWALK|  true|   false|1522|      15|  29|            25|      18|     1141717|     1898581|2015|02/15/2015 12:43:...| 41.87777333|-87.755117993|(41.87777333, -87...|\n",
      "|9955846|   HY144829|02/08/2015 11:30:...|0000X S MAYFIELD AVE|0610|    BURGLARY|      FORCIBLE ENTRY|           APARTMENT| false|   false|1513|      15|  29|            25|      05|     1137239|     1899372|2015|02/15/2015 12:43:...|41.880025548|-87.771541324|(41.880025548, -8...|\n",
      "|9955835|   HY144778|02/08/2015 11:30:...|     010XX W 48TH ST|0486|     BATTERY|DOMESTIC BATTERY ...|           APARTMENT| false|    true| 933|       9|   3|            61|     08B|     1169986|     1873019|2015|02/15/2015 12:43:...|41.807059405| -87.65206589|(41.807059405, -8...|\n",
      "+-------+-----------+--------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import udf\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#dat = (sqlContext.read.format(\"com.databricks.spark.csv\").options(header= \"true\",delimiter=',',nullValue=\"null\").load(\"/user-home/1014/Jeff_data_pull_20180209.csv\"))\n",
    "\n",
    "dat = (sqlContext.read.csv(path=\"/user-home/1014/chicago_crime.csv\",header=True,sep=',',inferSchema=True))\n",
    "dat=dat.dropna()\n",
    "print(type(dat))\n",
    "dat.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Community Area', 'int'), ('X Coordinate', 'int'), ('Y Coordinate', 'int')]\n",
      "+--------------+------------+------------+\n",
      "|Community Area|X Coordinate|Y Coordinate|\n",
      "+--------------+------------+------------+\n",
      "|            46|     1198273|     1851626|\n",
      "|            53|     1178335|     1826581|\n",
      "|            25|     1141717|     1898581|\n",
      "|            25|     1137239|     1899372|\n",
      "|            61|     1169986|     1873019|\n",
      "+--------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "skinny_dat=dat.select(['Community Area','X Coordinate','Y Coordinate'])\n",
    "skinny_dat=skinny_dat.fillna(0)\n",
    "print(skinny_dat.dtypes)\n",
    "skinny_dat.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def udf_wrapper(returntype):\n",
    "\n",
    "    def udf_func(func):\n",
    "\n",
    "        return udf(func, returnType=returntype)\n",
    "\n",
    "    return udf_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "@udf_wrapper(StringType())\n",
    "def community_over_40(x):\n",
    "\n",
    "    return 'Yes' if x>40 else 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+------------+-------+\n",
      "|Community Area|X Coordinate|Y Coordinate|comm_40|\n",
      "+--------------+------------+------------+-------+\n",
      "|            46|     1198273|     1851626|    Yes|\n",
      "|            53|     1178335|     1826581|    Yes|\n",
      "|            25|     1141717|     1898581|     No|\n",
      "|            25|     1137239|     1899372|     No|\n",
      "|            61|     1169986|     1873019|    Yes|\n",
      "+--------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "skinny_dat = skinny_dat.withColumn(\"comm_40\", community_over_40(skinny_dat['Community Area']))\n",
    "skinny_dat.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Group by agg examples</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|comm_40|min(X Coordinate)|\n",
      "+-------+-----------------+\n",
      "|     No|          1119397|\n",
      "|    Yes|          1100317|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#group by agg example\n",
    "\n",
    "skinny_dat.groupby('comm_40').agg(F.min('X Coordinate')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Community Area=46, X Coordinate=1198273, Y Coordinate=1851626, comm_40=u'Yes'), Row(Community Area=53, X Coordinate=1178335, Y Coordinate=1826581, comm_40=u'Yes'), Row(Community Area=25, X Coordinate=1141717, Y Coordinate=1898581, comm_40=u'No'), Row(Community Area=25, X Coordinate=1137239, Y Coordinate=1899372, comm_40=u'No'), Row(Community Area=61, X Coordinate=1169986, Y Coordinate=1873019, comm_40=u'Yes'), Row(Community Area=1, X Coordinate=1164732, Y Coordinate=1943222, comm_40=u'No'), Row(Community Area=19, X Coordinate=1135910, Y Coordinate=1914206, comm_40=u'No'), Row(Community Area=32, X Coordinate=1175384, Y Coordinate=1902088, comm_40=u'No'), Row(Community Area=32, X Coordinate=1175384, Y Coordinate=1902088, comm_40=u'No'), Row(Community Area=6, X Coordinate=1168809, Y Coordinate=1921459, comm_40=u'No')]\n",
      "+-------+--------------------+\n",
      "|comm_40|              list_X|\n",
      "+-------+--------------------+\n",
      "|     No|[1139776, 1177758...|\n",
      "|    Yes|[1170585, 1189089...|\n",
      "+-------+--------------------+\n",
      "\n",
      "None\n",
      "+---+-------+\n",
      "| _1|     _2|\n",
      "+---+-------+\n",
      "| No|1119397|\n",
      "|Yes|1100317|\n",
      "+---+-------+\n",
      "\n",
      "None\n",
      "+-------+--------------------+-------+\n",
      "|comm_40|              list_X|  min_x|\n",
      "+-------+--------------------+-------+\n",
      "|     No|[1139776, 1177758...|1119397|\n",
      "|    Yes|[1170585, 1189089...|1100317|\n",
      "+-------+--------------------+-------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#same, using UDF\n",
    "#https://stackoverflow.com/questions/46187630/how-to-write-pyspark-udaf-on-multiple-columns\n",
    "\n",
    "#quite complicated as no user defined agg function support in pyspark (the dev version does though)\n",
    "#https://spark.apache.org/docs/1.1.1/api/python/pyspark.rdd.RDD-class.html#zipWithIndex\n",
    "\n",
    "#so try with RDD\n",
    "print(skinny_dat.rdd.take(10))\n",
    "\n",
    "\n",
    "v=(skinny_dat\n",
    "  .groupby(\"comm_40\")\n",
    "  .agg(F.collect_set(\"X Coordinate\").alias(\"list_X\")))\n",
    "\n",
    "print(v.show())\n",
    "\n",
    "\n",
    "z=v.rdd.map(lambda x: (x[0],min(x[1])))\n",
    "print(z.toDF().show())\n",
    "\n",
    "#or\n",
    "\n",
    "\n",
    "def calc_min(data_list):\n",
    "    \n",
    "    min =data_list[0]\n",
    "    \n",
    "    for l in data_list:\n",
    "        if l <min:\n",
    "            min=l\n",
    "    \n",
    "    return min\n",
    "\n",
    "calc_min_udf = udf(calc_min, IntegerType() )\n",
    "\n",
    "z=v.withColumn('min_x', calc_min_udf('list_X'))\n",
    "print(z.show())\n",
    "\n",
    "#couldnt figure out how to pass rdd to a udf that was not lamnbda - maybe just tired :)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>SQL join and case statement</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "user_schema = StructType([StructField(\"date\", DateType(), True),\n",
    "                               StructField(\"user\", IntegerType(), True),\n",
    "                         StructField(\"type\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Type: ', <class 'pyspark.sql.dataframe.DataFrame'>)\n",
      "('Data Types: ', [('date', 'date'), ('user', 'int'), ('type', 'string')])\n",
      "('Number of rows: ', 9)\n",
      "+----------+----+------------+\n",
      "|      date|user|        type|\n",
      "+----------+----+------------+\n",
      "|2017-01-12| 123|Email_change|\n",
      "|2018-01-12| 123|Phone_change|\n",
      "|2017-02-28| 123|Email_change|\n",
      "|2017-02-13| 123|Email_change|\n",
      "|2017-01-12| 456|Email_change|\n",
      "|2017-01-12| 789|   NA_Change|\n",
      "|2017-01-15| 789|   NA_Change|\n",
      "|2017-01-12| 789|   NA_Change|\n",
      "|2017-01-12| 123|Phone_change|\n",
      "+----------+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "tbl = (sqlContext.read.format(\"com.databricks.spark.csv\").options(header= \"true\",delimiter=',',dateFormat=\"MM/dd/yyyy\").schema(user_schema).load(\"/user-home/1014/test_sql_join.csv\"))\n",
    "print(\"Type: \",type(tbl))\n",
    "print(\"Data Types: \",tbl.dtypes)\n",
    "print('Number of rows: ',tbl.count())\n",
    "tbl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tbl.registerTempTable(\"tbl_1\")\n",
    "tbl.registerTempTable(\"tbl_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "join logic:\n",
    "\n",
    "same user\n",
    "the event to consider happens in same or a later month after the current row \n",
    "the event to consider happens at most 3 month after the current row \n",
    "the event to consider happens after the current row in terms of days \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "months_lookback = 3\n",
    "\n",
    "\n",
    "query=            'select a.user, a.date, a.type, \\\n",
    "                  min(case when b.type = \"Email_change\" then datediff(b.date, a.date) end) as days_since_emails \\\n",
    "                  from tbl_1 a  left outer join tbl_2 b  \\\n",
    "                  on a.user=b.user  \\\n",
    "                  and months_between(b.date,a.date) >=0  \\\n",
    "                  and months_between(b.date,a.date) <=\"{}\"  \\\n",
    "                  and b.date > a.date   \\\n",
    "                  group by a.user, a.date, a.type'.format(months_lookback)\n",
    "\n",
    "\n",
    "sqlDf = spark.sql(query)\n",
    "\n",
    "#sqlDf=sqlDf.fillna(months_lookback +1)                  \n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "+----+----------+------------+-----------------+\n",
      "|user|      date|        type|days_since_emails|\n",
      "+----+----------+------------+-----------------+\n",
      "| 789|2017-01-12|   NA_Change|             null|\n",
      "| 123|2018-01-12|Phone_change|             null|\n",
      "| 123|2017-01-12|Phone_change|               32|\n",
      "| 123|2017-01-12|Email_change|               32|\n",
      "| 123|2017-02-13|Email_change|               15|\n",
      "| 789|2017-01-15|   NA_Change|             null|\n",
      "| 123|2017-02-28|Email_change|             null|\n",
      "| 456|2017-01-12|Email_change|             null|\n",
      "+----+----------+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sqlDf.count())\n",
    "sqlDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Play with RDD</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date=datetime.date(2017, 1, 12), user=123, type=u'Email_change'),\n",
       " Row(date=datetime.date(2018, 1, 12), user=123, type=u'Phone_change'),\n",
       " Row(date=datetime.date(2017, 2, 28), user=123, type=u'Email_change'),\n",
       " Row(date=datetime.date(2017, 2, 13), user=123, type=u'Email_change'),\n",
       " Row(date=datetime.date(2017, 1, 12), user=456, type=u'Email_change')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl.rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[123, 123, 123, 123, 456]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#col\n",
    "tbl.rdd.map(lambda row: row[1]).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "['a', 'b', 'c', 'd']\n",
      "[('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 0, 1), ('b', 1, 2), ('c', 2, 3), ('d', 3, 4)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one=sc.parallelize([\"a\", \"b\", \"c\", \"d\"])\n",
    "print(type(one))\n",
    "print(one.take(10))\n",
    "two=one.zipWithIndex()\n",
    "print(two.take(10))\n",
    "\n",
    "\n",
    "three=two.map(lambda x: (x[0],x[1],x[1]+1))\n",
    "three.take(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_1', 'string'), ('_2', 'array<string>')]\n",
      "+---+--------------------+\n",
      "| _1|                  _2|\n",
      "+---+--------------------+\n",
      "|  a|[apple, banana, l...|\n",
      "|  b|            [grapes]|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
    "x.values().collect()\n",
    "y=x.toDF()\n",
    "print(y.dtypes)\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('id', 'string'), ('code', 'bigint'), ('index', 'bigint')]\n",
      "+---+----+-----+\n",
      "| id|code|index|\n",
      "+---+----+-----+\n",
      "|  a|   1|    2|\n",
      "|  a|   1|    4|\n",
      "|  b|null|    6|\n",
      "+---+----+-----+\n",
      "\n",
      "None\n",
      "+---+-----------------+-------------------+\n",
      "| id|collect_set(code)|collect_list(index)|\n",
      "+---+-----------------+-------------------+\n",
      "|  b|               []|                [6]|\n",
      "|  a|              [1]|             [2, 4]|\n",
      "+---+-----------------+-------------------+\n",
      "\n",
      "None\n",
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  b|  6|\n",
      "|  a|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame([\n",
    "    (\"a\", 1, 2),\n",
    "    (\"a\", 1, 4),\n",
    "    (\"b\", None, 6),\n",
    "], [\"id\", \"code\", \"index\"])\n",
    "print(df.dtypes)\n",
    "print(df.show())\n",
    "\n",
    "v=(df\n",
    "  .groupby(\"id\")\n",
    "  .agg(F.collect_set(\"code\"), #will remove dups\n",
    "       F.collect_list(\"index\")))\n",
    "\n",
    "print(v.show())\n",
    "\n",
    "\n",
    "z=v.rdd.map(lambda x: (x[0],min(x[2])))\n",
    "z.toDF().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>windowing examples</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+-----------------+\n",
      "|      date|var1|var2|dayssinceJan11900|\n",
      "+----------+----+----+-----------------+\n",
      "|2017-01-30| 123|   A|            42763|\n",
      "|2017-01-17| 123|   B|            42750|\n",
      "|2017-01-15| 123|   A|            42748|\n",
      "|2017-01-15| 123|   A|            42748|\n",
      "|2017-01-14| 123|   A|            42747|\n",
      "|2017-01-11| 123|   B|            42744|\n",
      "|2017-01-29| 456|   A|            42762|\n",
      "|2017-01-22| 789|   B|            42755|\n",
      "|2017-01-21| 789|   B|            42754|\n",
      "|2017-01-20| 789|   A|            42753|\n",
      "+----------+----+----+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "\n",
    "_schema = StructType([StructField(\"date\", StringType(), True),\n",
    "                               StructField(\"var1\", IntegerType(), True),\n",
    "                         StructField(\"var2\", StringType(), True)])\n",
    "\n",
    "\n",
    "test_list = [('2017-01-30',123,'A'),\n",
    "('2017-01-17',123,'B'),\n",
    "('2017-01-15',123,'A'),\n",
    "('2017-01-15',123,'A'),\n",
    "('2017-01-14',123,'A'),\n",
    "('2017-01-11',123,'B'),\n",
    "('2017-01-29',456,'A'),\n",
    "('2017-01-22',789,'B'),\n",
    "('2017-01-21',789,'B'),\n",
    "('2017-01-20',789,'A'),\n",
    "('2017-01-19',789,'A')\n",
    "\n",
    "]\n",
    "\n",
    "df = sqlContext.createDataFrame(test_list,schema=_schema) \n",
    "df=(df.withColumn('date',df.date.cast(DateType())))\n",
    "\n",
    "#add this to have a numeric to use below \n",
    "df=(df.withColumn('dayssinceJan11900',datediff(df.date,F.lit(date(1900, 1, 1)))))\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+-----------------+-----------------+\n",
      "|      date|var1|var2|dayssinceJan11900|events_past_3days|\n",
      "+----------+----+----+-----------------+-----------------+\n",
      "|2017-01-29| 456|   A|            42762|                0|\n",
      "|2017-01-19| 789|   A|            42752|                0|\n",
      "|2017-01-20| 789|   A|            42753|                1|\n",
      "|2017-01-21| 789|   B|            42754|                2|\n",
      "|2017-01-22| 789|   B|            42755|                3|\n",
      "|2017-01-11| 123|   B|            42744|                0|\n",
      "|2017-01-14| 123|   A|            42747|                1|\n",
      "|2017-01-15| 123|   A|            42748|                1|\n",
      "|2017-01-15| 123|   A|            42748|                1|\n",
      "|2017-01-17| 123|   B|            42750|                3|\n",
      "|2017-01-30| 123|   A|            42763|                0|\n",
      "+----------+----+----+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#count number of times each user had event in past 3 days\n",
    "\n",
    "wSpec1=Window.partitionBy('var1').orderBy('dayssinceJan11900').rangeBetween(-3 ,-1)\n",
    "\n",
    "df=(df.withColumn(\"events_past_3days\", F.count(df.var2).over(wSpec1)))\n",
    "df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+-----------------+-----------------+-------------------+\n",
      "|      date|var1|var2|dayssinceJan11900|events_past_3days|A_events_past_3days|\n",
      "+----------+----+----+-----------------+-----------------+-------------------+\n",
      "|2017-01-29| 456|   A|            42762|                0|               null|\n",
      "|2017-01-19| 789|   A|            42752|                0|               null|\n",
      "|2017-01-20| 789|   A|            42753|                1|                  1|\n",
      "|2017-01-21| 789|   B|            42754|                2|                  2|\n",
      "|2017-01-22| 789|   B|            42755|                3|                  2|\n",
      "|2017-01-11| 123|   B|            42744|                0|               null|\n",
      "|2017-01-14| 123|   A|            42747|                1|                  0|\n",
      "|2017-01-15| 123|   A|            42748|                1|                  1|\n",
      "|2017-01-15| 123|   A|            42748|                1|                  1|\n",
      "|2017-01-17| 123|   B|            42750|                3|                  3|\n",
      "|2017-01-30| 123|   A|            42763|                0|               null|\n",
      "+----------+----+----+-----------------+-----------------+-------------------+\n",
      "\n",
      "+----------+----+----+-----------------+-----------------+-------------------+\n",
      "|      date|var1|var2|dayssinceJan11900|events_past_3days|A_events_past_3days|\n",
      "+----------+----+----+-----------------+-----------------+-------------------+\n",
      "|2017-01-29| 456|   A|            42762|                0|                  0|\n",
      "|2017-01-19| 789|   A|            42752|                0|                  0|\n",
      "|2017-01-20| 789|   A|            42753|                1|                  1|\n",
      "|2017-01-21| 789|   B|            42754|                2|                  2|\n",
      "|2017-01-22| 789|   B|            42755|                3|                  2|\n",
      "|2017-01-11| 123|   B|            42744|                0|                  0|\n",
      "|2017-01-14| 123|   A|            42747|                1|                  0|\n",
      "|2017-01-15| 123|   A|            42748|                1|                  1|\n",
      "|2017-01-15| 123|   A|            42748|                1|                  1|\n",
      "|2017-01-17| 123|   B|            42750|                3|                  3|\n",
      "|2017-01-30| 123|   A|            42763|                0|                  0|\n",
      "+----------+----+----+-----------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#count number of times each user had event in past 3 days of type A\n",
    "df=(df.withColumn(\"A_events_past_3days\", F.sum((df.var2==('A')).cast(IntegerType())).over(wSpec1)))\n",
    "df.show()\n",
    "\n",
    "#null is returned when there are no records prior (in range) to a row\n",
    "df=df.fillna(0,'A_events_past_3days')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#count number of times each user had event in past 3 days when event is same as the current row\n",
    "#????????#????????????????????????????????????????????????\n",
    "\n",
    "df=(df.withColumn(\"same_events_past_3days\", ).over(wSpec1)))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+-----------------+----------------------+\n",
      "|      date|var1|var2|dayssinceJan11900|events_past_3days_list|\n",
      "+----------+----+----+-----------------+----------------------+\n",
      "|2017-01-29| 456|   A|            42762|                    []|\n",
      "|2017-01-19| 789|   A|            42752|                    []|\n",
      "|2017-01-20| 789|   A|            42753|                   [A]|\n",
      "|2017-01-21| 789|   B|            42754|                [A, A]|\n",
      "|2017-01-22| 789|   B|            42755|             [A, A, B]|\n",
      "|2017-01-11| 123|   B|            42744|                    []|\n",
      "|2017-01-14| 123|   A|            42747|                   [B]|\n",
      "|2017-01-15| 123|   A|            42748|                   [A]|\n",
      "|2017-01-15| 123|   A|            42748|                   [A]|\n",
      "|2017-01-17| 123|   B|            42750|             [A, A, A]|\n",
      "|2017-01-30| 123|   A|            42763|                    []|\n",
      "+----------+----+----+-----------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wSpec1=Window.partitionBy('var1').orderBy('dayssinceJan11900').rangeBetween(-3 ,-1)\n",
    "\n",
    "#keept the set to see what is there\n",
    "df=(df.withColumn(\"events_past_3days_list\", F.collect_list(df.var2).over(wSpec1)))\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
