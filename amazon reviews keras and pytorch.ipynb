{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jma/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/jma/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "PATH = \"/home/jma/Desktop/march_embedding/\"\n",
    "\n",
    "import keras\n",
    "from keras import *\n",
    "from keras.layers import Embedding, Input,concatenate, Flatten,dot, Dense,LSTM\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as utils_data\n",
    "from torch.autograd import Variable\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import Dataset \n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_normal, xavier_uniform\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "use_gpu=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common parameters\n",
    "MAX_WORDS=10000 #keep top words\n",
    "MAX_SEQ=200  #max length of sequence \n",
    "LSTM_SIZE = 128 #output size of LSTM\n",
    "EMBEDDING_SIZE=128 #size of embedding \n",
    "DENSE_LAYER = 25 #fc1 output size\n",
    "BATCH_SIZE=64 \n",
    "EPOCHS=2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load and prepare the data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('/media/jma/DATA/amazon_review_full_csv/train.csv',header=0,nrows=250000,names=['rating','short','fullreview'])\n",
    "y=train.rating.values\n",
    "\n",
    "y=np.where(y>3,1,0)\n",
    "\n",
    "tokenizer=Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(train.fullreview)\n",
    "X=tokenizer.texts_to_sequences(train.fullreview)\n",
    "X=pad_sequences(X,maxlen=MAX_SEQ,padding=\"pre\") #padding on the left, this seems to matter to pytorch \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>KERAS</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 200, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 25)                3225      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 1,414,835\n",
      "Trainable params: 1,414,835\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 979s 5ms/step - loss: 0.4113\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 968s 5ms/step - loss: 0.3305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7effc19add68>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embedding_layer = Embedding(output_dim=EMBEDDING_SIZE, input_dim=MAX_WORDS,input_length=MAX_SEQ, mask_zero=True)\n",
    "\n",
    "x = Input(shape=[MAX_SEQ])\n",
    "embedding = embedding_layer(x)\n",
    "lstm_out=LSTM(LSTM_SIZE)(embedding)\n",
    "fc1=Dense(DENSE_LAYER,activation='relu')(lstm_out)\n",
    "out=Dense(1,activation=\"sigmoid\")(fc1)\n",
    "\n",
    "model = Model(inputs=x, outputs=out)\n",
    "\n",
    "print(model.summary())\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
    "\n",
    "model.fit(x=X_train, y=y_train, batch_size=BATCH_SIZE,epochs=EPOCHS, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9207524993337503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.84844"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=model.predict(X_test)\n",
    "\n",
    "print(roc_auc_score(y_test, pred))\n",
    "accuracy_score(y_test, np.where(pred>0.5,1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pytorch</h2>\n",
    "<br/>\n",
    "https://docs.google.com/presentation/d/17VUX7YXhMkJrqO5gNGh6EE5gzBpY-BF9IrfVKcFIb3A/edit#slide=id.g27e9c2914b_0_460\n",
    "<br/>\n",
    "<br/>\n",
    "https://github.com/hunkim/PyTorchZeroToAll/blob/master/13_1_rnn_classification_basics.py\n",
    "<br/>\n",
    "<br/>\n",
    "https://github.com/yuchenlin/lstm_sentence_classifier/blob/master/LSTM_sentence_classifier_minibatch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samples = utils_data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "data_loader = utils_data.DataLoader(training_samples, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, batch_size, hidden_dim, num_embeddings,embedding_dim, dense_layer_dim, use_gpu):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.hidden_dim = hidden_dim #(output size of the LSTM)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.dense_layer_dim=dense_layer_dim\n",
    "        self.use_gpu =use_gpu\n",
    "        \n",
    "                \n",
    "        self.embeddings = nn.Embedding(num_embeddings=self.num_embeddings,embedding_dim=self.embedding_dim)\n",
    "        \n",
    "        #hidden size is the output size of the LSTM\n",
    "        #batch_first â€“ If True, then the input and output tensors are provided as (batch, seq, feature)\n",
    "        self.lstm  =nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_dim ,batch_first=True )\n",
    "        self.fc1   = nn.Linear(in_features=self.hidden_dim, out_features=self.dense_layer_dim)\n",
    "        self.fc2   = nn.Linear(in_features=self.dense_layer_dim,out_features=1)\n",
    "         \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Make a hidden\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        x=self.embeddings(x)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden) #hidden is a tuple\n",
    "   \n",
    "        #I think this is correct and is getting all the batches, the last output and the full length of the hidden dim\n",
    "        x  = F.relu(self.fc1(lstm_out[:,-1,:]))\n",
    "        out = F.sigmoid(self.fc2(x))\n",
    "       \n",
    "        return out\n",
    "    \n",
    "\n",
    "        \n",
    "        # Initialize hidden and cell states\n",
    "    def init_hidden(self, batch_size):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        \n",
    "        #both are size (num_layers * num_directions, batch, hidden_size)\n",
    "        \n",
    "        h=Variable(torch.zeros(1, batch_size, self.hidden_dim))\n",
    "        c=Variable(torch.zeros(1, batch_size, self.hidden_dim))\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            h,c = h.cuda(), c.cuda() \n",
    "        \n",
    "        return (h,c)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (embeddings): Embedding(10000, 128)\n",
      "  (lstm): LSTM(128, 128, batch_first=True)\n",
      "  (fc1): Linear(in_features=128, out_features=25, bias=True)\n",
      "  (fc2): Linear(in_features=25, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jma/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1474: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/home/jma/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "model=Model(batch_size=BATCH_SIZE, \n",
    "        hidden_dim= LSTM_SIZE, \n",
    "        num_embeddings = MAX_WORDS,\n",
    "        embedding_dim=EMBEDDING_SIZE, \n",
    "        dense_layer_dim=DENSE_LAYER,\n",
    "        use_gpu =use_gpu\n",
    "        )\n",
    "\n",
    "if use_gpu:\n",
    "    model.cuda()\n",
    "print(model)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-3,betas=(0.9, 0.999))\n",
    "# create a loss function (mse)\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "# run the main training loop\n",
    "hold_loss=[]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    cum_loss=0.\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        #EMBEDDING NEEDS INPUT THAT IS A LONG \n",
    "        tr_x, tr_y = Variable(data.long()), Variable(target.float().view(target.shape[0],))\n",
    "        if use_gpu:\n",
    "            tr_x, tr_y = tr_x.cuda(), tr_y.cuda() \n",
    "           \n",
    "        # Reset gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        fx = model(tr_x)\n",
    "        \n",
    "        output = loss(fx, tr_y) #loss for this batch\n",
    "        cum_loss += output.data[0] #accumulate the loss\n",
    "        \n",
    "        # Backward \n",
    "        output.backward()\n",
    "        \n",
    "        # Update parameters based on backprop\n",
    "        optimizer.step()\n",
    "        \n",
    "    hold_loss.append(cum_loss/len(data_loader))    \n",
    "    #print(epoch+1, cum_loss) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9171270064199741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.84334"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples = utils_data.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "data_loader_test = utils_data.DataLoader(test_samples, batch_size=64)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(data_loader_test):\n",
    "    #EMBEDDING NEEDS INPUT THAT IS A LONG \n",
    "    tst_x, tst_y = Variable(data.long()), Variable(target.float().view(target.shape[0],))\n",
    "    if use_gpu:\n",
    "        tst_x, tst_y = tst_x.cuda(), tst_y.cuda() \n",
    "    \n",
    "    pred=model(tst_x)\n",
    "    \n",
    "    if use_gpu:\n",
    "        pred = pred.cpu().data.numpy()\n",
    "        tst_y = tst_y.cpu().data.numpy().reshape(tst_y.shape[0],1)\n",
    "\n",
    "    if batch_idx ==0:\n",
    "        hold_pred=pred\n",
    "        hold_actual=tst_y\n",
    "    \n",
    "    else:\n",
    "        hold_pred =np.row_stack([hold_pred,pred])\n",
    "        hold_actual =np.row_stack([hold_actual,tst_y])\n",
    "\n",
    "print(roc_auc_score(hold_actual, hold_pred))\n",
    "accuracy_score(hold_actual, np.where(hold_pred>0.5,1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pytorch again, this time use packing to mask zero like Keras</h2>\n",
    "<br/>\n",
    "<br/>\n",
    "https://medium.com/huggingface/understanding-emotions-from-keras-to-pytorch-3ccb61d5a983\n",
    "<br/>\n",
    "https://github.com/hunkim/PyTorchZeroToAll/blob/master/13_4_pack_pad.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will need to have zero padded like before BUT right padded\n",
    "\n",
    "train=pd.read_csv('/media/jma/DATA/amazon_review_full_csv/train.csv',header=0,nrows=250000,names=['rating','short','fullreview'])\n",
    "y=train.rating.values\n",
    "\n",
    "y=np.where(y>3,1,0)\n",
    "\n",
    "tokenizer=Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(train.fullreview)\n",
    "X=tokenizer.texts_to_sequences(train.fullreview)\n",
    "X=pad_sequences(X,maxlen=MAX_SEQ,padding=\"pre\") #padding on the left, this seems to matter to pytorch \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "#errors in the sortseq happen with all zero rows\n",
    "zero_x_train = ~np.all(X_train == 0, axis=1)\n",
    "zero_x_test = ~np.all(X_test == 0, axis=1)\n",
    "\n",
    "X_train=X_train[zero_x_train]\n",
    "y_train=y_train[zero_x_train]\n",
    "\n",
    "X_test=X_test[zero_x_test]\n",
    "y_test=y_test[zero_x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    7,    8,   27,  175,\n",
       "          5,  909,    1, 9568, 3981,    2,  175,    5,  909, 6297,  265,\n",
       "       1201,  334,   14,   15, 1465,   73,   97,  430, 1237,  298,    7,\n",
       "          8,  397], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samples = utils_data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "data_loader = utils_data.DataLoader(training_samples, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortseq(x,y):\n",
    "  #input is (batch,seq_length (padded by zero))  \n",
    "  \n",
    "    input_lengths = torch.LongTensor([torch.max(x[i, :].data.nonzero()) + 1 for i in range(x.size()[0])])\n",
    "    #A tuple of (sorted_tensor, sorted_indices) is returned, where the sorted_indices are the indices of the elements in the original input tensor.\n",
    "    input_lengths, perm_idx = input_lengths.sort(0, descending=True)\n",
    "    x = x[perm_idx]\n",
    "    y= y[perm_idx]\n",
    "\n",
    "    return (x,y,input_lengths.cpu().numpy())\n",
    "\n",
    "\n",
    "#sort the batch by decreasing sequence length\n",
    "#x=np.array([[1, 2,0], [0, 0, 0], [3, 4, 5]])\n",
    "#x=Variable(torch.from_numpy(x)).long()\n",
    "#sortseq(x)\n",
    "\n",
    "#x = x[(x != 0).any()]\n",
    "\n",
    "#input_lengths=torch.LongTensor([torch.max(x[i,:].data.nonzero()) +1 for i in range((x.size()[0]))]) #loops through each instance in x and records the index \n",
    "#input_lengths, perm_idx = input_lengths.sort(0, descending=True)\n",
    "\n",
    "#g=x[perm_idx]\n",
    "#g\n",
    "\n",
    "\n",
    "#h=nn.Embedding(10,2)\n",
    "#h=h(g)\n",
    "#pack=pack_padded_sequence(h, input_lengths.cpu().numpy())\n",
    "#pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, batch_size, hidden_dim, num_embeddings,embedding_dim, dense_layer_dim, use_gpu):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.hidden_dim = hidden_dim #(output size of the LSTM)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.dense_layer_dim=dense_layer_dim\n",
    "        self.use_gpu =use_gpu\n",
    "        \n",
    "       \n",
    "                \n",
    "        self.embeddings = nn.Embedding(num_embeddings=self.num_embeddings,embedding_dim=self.embedding_dim)\n",
    "        \n",
    "        #hidden size is the output size of the LSTM\n",
    "        #batch_first â€“ If True, then the input and output tensors are provided as (batch, seq, feature)\n",
    "        self.lstm  =nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_dim ,batch_first=True )\n",
    "        self.fc1   = nn.Linear(in_features=self.hidden_dim, out_features=self.dense_layer_dim)\n",
    "        self.fc2   = nn.Linear(in_features=self.dense_layer_dim,out_features=1)\n",
    "         \n",
    "\n",
    "    def forward(self, x, input_lengths): #THIS IS NEW FOR MASKIN\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Make a hidden\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "                \n",
    "        x=self.embeddings(x)\n",
    "        \n",
    "        #THIS IS NEW FOR MASKING#####################################################\n",
    "        #Pack the zero padded and sorted embedding\n",
    "        x = pack_padded_sequence(x, input_lengths.data.cpu().numpy(), batch_first=True)\n",
    "        \n",
    "        #################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden) #hidden is a tuple\n",
    "   \n",
    "\n",
    "        #THIS IS NEW FOR MASKING#####################################################\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out,batch_first=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #################################################################\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        #I think this is correct and is getting all the batches, the last output and the full length of the hidden dim\n",
    "        x  = F.relu(self.fc1(lstm_out[:,-1,:]))\n",
    "        out = F.sigmoid(self.fc2(x))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "        \n",
    "        # Initialize hidden and cell states\n",
    "    def init_hidden(self, batch_size):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        \n",
    "        #both are size (num_layers * num_directions, batch, hidden_size)\n",
    "        \n",
    "        h=Variable(torch.zeros(1, batch_size, self.hidden_dim))\n",
    "        c=Variable(torch.zeros(1, batch_size, self.hidden_dim))\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            h,c = h.cuda(), c.cuda() \n",
    "        \n",
    "        return (h,c)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (embeddings): Embedding(10000, 128)\n",
      "  (lstm): LSTM(128, 128, batch_first=True)\n",
      "  (fc1): Linear(in_features=128, out_features=25, bias=True)\n",
      "  (fc2): Linear(in_features=25, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jma/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1474: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/home/jma/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/jma/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1474: UserWarning: Using a target size (torch.Size([60])) that is different to the input size (torch.Size([60, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    }
   ],
   "source": [
    "model=Model(batch_size=BATCH_SIZE, \n",
    "        hidden_dim= LSTM_SIZE, \n",
    "        num_embeddings = MAX_WORDS,\n",
    "        embedding_dim=EMBEDDING_SIZE, \n",
    "        dense_layer_dim=DENSE_LAYER,\n",
    "        use_gpu =use_gpu\n",
    "        )\n",
    "\n",
    "if use_gpu:\n",
    "    model.cuda()\n",
    "print(model)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-3,betas=(0.9, 0.999))\n",
    "# create a loss function (mse)\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "# run the main training loop\n",
    "hold_loss=[]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    cum_loss=0.\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        #EMBEDDING NEEDS INPUT THAT IS A LONG \n",
    "        tr_x, tr_y = Variable(data.long()), Variable(target.float().view(target.shape[0],))\n",
    "        \n",
    "        #THIS IS NEW FOR MASKING#####################################################\n",
    "        #pack\n",
    "        tr_x, tr_y,input_lengths =sortseq(tr_x,tr_y)\n",
    "\n",
    "        input_lengths=Variable(torch.from_numpy(input_lengths))\n",
    "        ################################################################\n",
    "        \n",
    "        if use_gpu:\n",
    "            tr_x, tr_y = tr_x.cuda(), tr_y.cuda() \n",
    "           \n",
    "        # Reset gradient\n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        # Forward pass\n",
    "        fx = model(tr_x,input_lengths) #THIS IS NEW FOR MASKING\n",
    "        \n",
    "        output = loss(fx, tr_y) #loss for this batch\n",
    "        cum_loss += output.data[0] #accumulate the loss\n",
    "        \n",
    "        # Backward \n",
    "        output.backward()\n",
    "        \n",
    "        # Update parameters based on backprop\n",
    "        optimizer.step()\n",
    "          \n",
    "    hold_loss.append(cum_loss/len(data_loader))    \n",
    "    #print(epoch+1, cum_loss) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to score test do we need to pack / unpack?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9147818740967897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.83544"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples = utils_data.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "data_loader_test = utils_data.DataLoader(test_samples, batch_size=64)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(data_loader_test):\n",
    "    #EMBEDDING NEEDS INPUT THAT IS A LONG \n",
    "    tst_x, tst_y = Variable(data.long()), Variable(target.float().view(target.shape[0],))\n",
    "    \n",
    "    #THIS IS NEW FOR MASKING#####################################################\n",
    "    #pack\n",
    "    tst_x, tst_y,input_lengths =sortseq(tst_x,tst_y)\n",
    "\n",
    "    input_lengths=Variable(torch.from_numpy(input_lengths))\n",
    "    ################################################################\n",
    "    \n",
    "    \n",
    "    if use_gpu:\n",
    "        tst_x, tst_y = tst_x.cuda(), tst_y.cuda() \n",
    "    \n",
    "    pred=model(tst_x, input_lengths)#THIS IS NEW FOR MASKING#####################################################\n",
    "    \n",
    "    if use_gpu:\n",
    "        pred = pred.cpu().data.numpy()\n",
    "        tst_y = tst_y.cpu().data.numpy().reshape(tst_y.shape[0],1)\n",
    "\n",
    "    if batch_idx ==0:\n",
    "        hold_pred=pred\n",
    "        hold_actual=tst_y\n",
    "    \n",
    "    else:\n",
    "        hold_pred =np.row_stack([hold_pred,pred])\n",
    "        hold_actual =np.row_stack([hold_actual,tst_y])\n",
    "\n",
    "print(roc_auc_score(hold_actual, hold_pred))\n",
    "accuracy_score(hold_actual, np.where(hold_pred>0.5,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[18, 15, 16, ..., 12,  4, 15],\n",
       "        [15, 19, 19, ..., 14,  2, 17],\n",
       "        [ 1,  1, 12, ...,  6,  9,  7],\n",
       "        [13,  2,  2, ...,  9,  4, 16],\n",
       "        [ 7,  4, 16, ..., 16, 17,  5],\n",
       "        [13,  1,  3, ..., 11, 17, 15]],\n",
       "\n",
       "       [[ 8,  4, 14, ..., 19, 14,  7],\n",
       "        [ 9,  8, 19, ...,  8, 19,  9],\n",
       "        [ 3, 19, 15, ..., 13,  6, 15],\n",
       "        [ 6,  6, 14, ..., 14, 12,  7],\n",
       "        [18,  2,  1, ...,  7, 16, 14],\n",
       "        [ 7, 15, 11, ..., 16,  9, 12]],\n",
       "\n",
       "       [[12, 11,  9, ...,  6,  8,  5],\n",
       "        [ 2, 19,  8, ...,  6,  4,  4],\n",
       "        [19, 19,  6, ..., 13,  6, 12],\n",
       "        [16,  5, 14, ...,  4, 17, 11],\n",
       "        [11,  1,  7, ..., 14, 10,  2],\n",
       "        [ 4,  5,  2, ...,  2, 14,  7]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[15,  6, 13, ..., 17,  7,  2],\n",
       "        [17,  3, 18, ..., 18,  8, 15],\n",
       "        [ 3, 19, 17, ..., 14,  7,  1],\n",
       "        [ 8, 13, 17, ...,  1, 15,  5],\n",
       "        [19,  4, 12, ..., 11,  2,  4],\n",
       "        [13, 12,  9, ...,  2, 14, 15]],\n",
       "\n",
       "       [[ 6, 15, 14, ...,  9,  6, 10],\n",
       "        [ 7, 18, 16, ...,  1, 19,  8],\n",
       "        [ 4,  1, 19, ..., 14, 16, 18],\n",
       "        [ 9,  4,  2, ..., 14, 18,  2],\n",
       "        [ 9, 10, 13, ..., 19, 10, 14],\n",
       "        [ 8,  1, 16, ..., 17, 14, 10]],\n",
       "\n",
       "       [[12,  2,  1, ..., 12,  5, 17],\n",
       "        [13,  9,  1, ..., 18,  2,  6],\n",
       "        [ 5, 13, 13, ..., 13, 19,  2],\n",
       "        [10, 16,  1, ..., 10,  7,  8],\n",
       "        [ 4, 16, 12, ...,  1,  8, 18],\n",
       "        [ 1, 13,  7, ..., 12, 14,  5]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeff=np.random.randint(1,20,(25,6,10))\n",
    "jeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [5, 3, 0]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.utils.rnn\n",
    "pad_sequences(jeff,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
