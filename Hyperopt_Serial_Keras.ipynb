{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyperopt Tuning </h3>\n",
    "Could add early stopping with a validation set and return it back like this code:\n",
    "https://www.kaggle.com/inspector/keras-hyperopt-example-sketch\n",
    "\n",
    "and I think this shows how to get the number of rounds from https://stackoverflow.com/questions/49852241/return-number-of-epochs-for-earlystopping-callback-in-keras\n",
    "where I guess you would not use kfold and instead train on full train with a dedicated validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 544.4332\n",
      "Epoch 2/3\n",
      "337/337 [==============================] - 0s 69us/step - loss: 454.0971\n",
      "Epoch 3/3\n",
      "337/337 [==============================] - 0s 79us/step - loss: 375.3731\n",
      "Epoch 1/3\n",
      "337/337 [==============================] - 0s 67us/step - loss: 169.1701\n",
      "Epoch 2/3\n",
      "337/337 [==============================] - 0s 68us/step - loss: 141.6657\n",
      "Epoch 3/3\n",
      "337/337 [==============================] - 0s 75us/step - loss: 125.2757\n",
      "Epoch 1/3\n",
      "338/338 [==============================] - 0s 70us/step - loss: 205.7352\n",
      "Epoch 2/3\n",
      "338/338 [==============================] - 0s 72us/step - loss: 141.6986\n",
      "Epoch 3/3\n",
      "338/338 [==============================] - 0s 82us/step - loss: 111.9165\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 638.6276\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 0s 253us/step - loss: 188.8079\n",
      "Epoch 1/1\n",
      "338/338 [==============================] - 0s 249us/step - loss: 150.9365\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 541.0482\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 0s 159us/step - loss: 165.1585\n",
      "Epoch 1/1\n",
      "338/338 [==============================] - 0s 152us/step - loss: 171.5272\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 411.2617\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 248us/step - loss: 195.1431\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 258us/step - loss: 151.8730\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 257us/step - loss: 134.4933\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 275us/step - loss: 70.7366\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 272us/step - loss: 67.8545\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 287us/step - loss: 65.7911\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 282us/step - loss: 64.2430\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 241us/step - loss: 74.3272\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 249us/step - loss: 67.4276\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 249us/step - loss: 66.9682\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 253us/step - loss: 64.9143\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 726.6584\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 149us/step - loss: 361.7459\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 151us/step - loss: 183.8560\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 147us/step - loss: 148.3718\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 127us/step - loss: 84.1245\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 131us/step - loss: 75.1512\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 135us/step - loss: 71.2349\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 134us/step - loss: 68.8573\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 134us/step - loss: 82.6076\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 133us/step - loss: 72.0141\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 130us/step - loss: 69.8375\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 130us/step - loss: 67.7120\n",
      "Epoch 1/3\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 648.7910\n",
      "Epoch 2/3\n",
      "337/337 [==============================] - 0s 144us/step - loss: 569.4861\n",
      "Epoch 3/3\n",
      "337/337 [==============================] - 0s 174us/step - loss: 379.0359\n",
      "Epoch 1/3\n",
      "337/337 [==============================] - 0s 136us/step - loss: 100.4442\n",
      "Epoch 2/3\n",
      "337/337 [==============================] - 0s 147us/step - loss: 86.3767\n",
      "Epoch 3/3\n",
      "337/337 [==============================] - 0s 141us/step - loss: 81.2309\n",
      "Epoch 1/3\n",
      "338/338 [==============================] - 0s 135us/step - loss: 125.9918\n",
      "Epoch 2/3\n",
      "338/338 [==============================] - 0s 142us/step - loss: 80.9535\n",
      "Epoch 3/3\n",
      "338/338 [==============================] - 0s 144us/step - loss: 80.1176\n",
      "Epoch 1/3\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 415.6586\n",
      "Epoch 2/3\n",
      "337/337 [==============================] - 0s 265us/step - loss: 121.3979\n",
      "Epoch 3/3\n",
      "337/337 [==============================] - 0s 274us/step - loss: 97.4142\n",
      "Epoch 1/3\n",
      "337/337 [==============================] - 0s 267us/step - loss: 64.7611\n",
      "Epoch 2/3\n",
      "337/337 [==============================] - 0s 275us/step - loss: 58.3190\n",
      "Epoch 3/3\n",
      "337/337 [==============================] - 0s 269us/step - loss: 57.9308\n",
      "Epoch 1/3\n",
      "338/338 [==============================] - 0s 252us/step - loss: 72.3591\n",
      "Epoch 2/3\n",
      "338/338 [==============================] - 0s 263us/step - loss: 64.3763\n",
      "Epoch 3/3\n",
      "338/338 [==============================] - 0s 253us/step - loss: 62.0339\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 490.1995\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 257us/step - loss: 162.1356\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 259us/step - loss: 123.5873\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 259us/step - loss: 107.6184\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 260us/step - loss: 68.7974\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 253us/step - loss: 61.6899\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 257us/step - loss: 58.9311\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 260us/step - loss: 57.1702\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 258us/step - loss: 71.6124\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 267us/step - loss: 65.0020\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 274us/step - loss: 62.3864\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 269us/step - loss: 60.4073\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 227.4746\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 0s 235us/step - loss: 72.3693\n",
      "Epoch 1/1\n",
      "338/338 [==============================] - 0s 235us/step - loss: 79.8766\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 614.7249\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 0s 134us/step - loss: 385.5736\n",
      "Epoch 1/1\n",
      "338/338 [==============================] - 0s 125us/step - loss: 529.3896\n",
      "Epoch 1/2\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 478.6676\n",
      "Epoch 2/2\n",
      "337/337 [==============================] - 0s 132us/step - loss: 317.5026\n",
      "Epoch 1/2\n",
      "337/337 [==============================] - 0s 133us/step - loss: 89.7895\n",
      "Epoch 2/2\n",
      "337/337 [==============================] - 0s 131us/step - loss: 73.7167\n",
      "Epoch 1/2\n",
      "338/338 [==============================] - 0s 129us/step - loss: 112.2682\n",
      "Epoch 2/2\n",
      "338/338 [==============================] - 0s 132us/step - loss: 80.5245\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 611.8009\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 0s 266us/step - loss: 283.3555\n",
      "Epoch 1/1\n",
      "338/338 [==============================] - 0s 261us/step - loss: 207.5531\n",
      "Epoch 1/2\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 415.6216\n",
      "Epoch 2/2\n",
      "337/337 [==============================] - 0s 71us/step - loss: 245.2196\n",
      "Epoch 1/2\n",
      "337/337 [==============================] - 0s 67us/step - loss: 83.8083\n",
      "Epoch 2/2\n",
      "337/337 [==============================] - 0s 69us/step - loss: 75.6964\n",
      "Epoch 1/2\n",
      "338/338 [==============================] - 0s 80us/step - loss: 91.7685\n",
      "Epoch 2/2\n",
      "338/338 [==============================] - 0s 68us/step - loss: 76.6226\n",
      "Epoch 1/3\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 732.9703\n",
      "Epoch 2/3\n",
      "337/337 [==============================] - 0s 133us/step - loss: 510.5741\n",
      "Epoch 3/3\n",
      "337/337 [==============================] - 0s 143us/step - loss: 297.7248\n",
      "Epoch 1/3\n",
      "337/337 [==============================] - 0s 138us/step - loss: 102.7287\n",
      "Epoch 2/3\n",
      "337/337 [==============================] - 0s 135us/step - loss: 96.6020\n",
      "Epoch 3/3\n",
      "337/337 [==============================] - 0s 136us/step - loss: 90.9775\n",
      "Epoch 1/3\n",
      "338/338 [==============================] - 0s 130us/step - loss: 131.2775\n",
      "Epoch 2/3\n",
      "338/338 [==============================] - 0s 135us/step - loss: 83.4870\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338/338 [==============================] - 0s 133us/step - loss: 83.4485\n",
      "Epoch 1/3\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 642.3632\n",
      "Epoch 2/3\n",
      "337/337 [==============================] - 0s 203us/step - loss: 389.3749\n",
      "Epoch 3/3\n",
      "337/337 [==============================] - 0s 182us/step - loss: 201.3957\n",
      "Epoch 1/3\n",
      "337/337 [==============================] - 0s 176us/step - loss: 86.7819\n",
      "Epoch 2/3\n",
      "337/337 [==============================] - 0s 170us/step - loss: 78.2535\n",
      "Epoch 3/3\n",
      "337/337 [==============================] - 0s 153us/step - loss: 75.4705\n",
      "Epoch 1/3\n",
      "338/338 [==============================] - 0s 169us/step - loss: 109.9598\n",
      "Epoch 2/3\n",
      "338/338 [==============================] - 0s 171us/step - loss: 78.9196\n",
      "Epoch 3/3\n",
      "338/338 [==============================] - 0s 168us/step - loss: 77.1360\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 710.0507\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 0s 141us/step - loss: 353.6256\n",
      "Epoch 1/1\n",
      "338/338 [==============================] - 0s 142us/step - loss: 328.7378\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 571.1536\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 0s 80us/step - loss: 330.9583\n",
      "Epoch 1/1\n",
      "338/338 [==============================] - 0s 76us/step - loss: 467.8164\n",
      "Epoch 1/2\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 479.4414\n",
      "Epoch 2/2\n",
      "337/337 [==============================] - 0s 159us/step - loss: 251.2073\n",
      "Epoch 1/2\n",
      "337/337 [==============================] - 0s 149us/step - loss: 99.0903\n",
      "Epoch 2/2\n",
      "337/337 [==============================] - 0s 154us/step - loss: 86.9759\n",
      "Epoch 1/2\n",
      "338/338 [==============================] - 0s 152us/step - loss: 125.2546\n",
      "Epoch 2/2\n",
      "338/338 [==============================] - 0s 153us/step - loss: 84.5272\n",
      "Epoch 1/3\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 298.5413\n",
      "Epoch 2/3\n",
      "337/337 [==============================] - 0s 267us/step - loss: 158.4316\n",
      "Epoch 3/3\n",
      "337/337 [==============================] - 0s 290us/step - loss: 119.2493\n",
      "Epoch 1/3\n",
      "337/337 [==============================] - 0s 275us/step - loss: 60.9007\n",
      "Epoch 2/3\n",
      "337/337 [==============================] - 0s 274us/step - loss: 57.4385\n",
      "Epoch 3/3\n",
      "337/337 [==============================] - 0s 310us/step - loss: 55.9817\n",
      "Epoch 1/3\n",
      "338/338 [==============================] - 0s 295us/step - loss: 69.3675\n",
      "Epoch 2/3\n",
      "338/338 [==============================] - 0s 306us/step - loss: 62.7599\n",
      "Epoch 3/3\n",
      "338/338 [==============================] - 0s 305us/step - loss: 61.3695\n",
      "Epoch 1/2\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 396.8074\n",
      "Epoch 2/2\n",
      "337/337 [==============================] - 0s 255us/step - loss: 145.5020\n",
      "Epoch 1/2\n",
      "337/337 [==============================] - 0s 252us/step - loss: 73.9664\n",
      "Epoch 2/2\n",
      "337/337 [==============================] - 0s 243us/step - loss: 67.9332\n",
      "Epoch 1/2\n",
      "338/338 [==============================] - 0s 270us/step - loss: 76.9454\n",
      "Epoch 2/2\n",
      "338/338 [==============================] - 0s 266us/step - loss: 71.1532\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 500.5965\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 276us/step - loss: 157.9463\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 291us/step - loss: 106.2084\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 275us/step - loss: 91.7941\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 356us/step - loss: 59.6178\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 358us/step - loss: 56.4557\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 335us/step - loss: 54.9751\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 330us/step - loss: 54.8013\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 293us/step - loss: 64.3218\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 303us/step - loss: 60.7142\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 291us/step - loss: 59.5071\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 328us/step - loss: 58.0198\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 585.0832\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 278us/step - loss: 184.1900\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 275us/step - loss: 120.1023\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 282us/step - loss: 98.2042\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 502us/step - loss: 58.5171\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 310us/step - loss: 56.2010\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 287us/step - loss: 55.5666\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 297us/step - loss: 55.0494\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 285us/step - loss: 66.7632\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 288us/step - loss: 63.7287\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 286us/step - loss: 61.7312\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 289us/step - loss: 60.4155\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 593.0133\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 284us/step - loss: 212.0456\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 278us/step - loss: 127.4077\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 276us/step - loss: 110.9788\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 282us/step - loss: 64.9216\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 275us/step - loss: 63.6666\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 285us/step - loss: 61.7064\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 285us/step - loss: 60.1633\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 286us/step - loss: 75.3598\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 283us/step - loss: 67.6791\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 274us/step - loss: 66.0124\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 283us/step - loss: 63.8992\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 413.1231\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 277us/step - loss: 206.1298\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 280us/step - loss: 149.3509\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 286us/step - loss: 111.0141\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 286us/step - loss: 65.7686\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 280us/step - loss: 62.3076\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 278us/step - loss: 60.7438\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 295us/step - loss: 60.3419\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 286us/step - loss: 74.2647\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 272us/step - loss: 65.4347\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 276us/step - loss: 65.2762\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 282us/step - loss: 62.6276\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 390.2617\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 284us/step - loss: 173.7679\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 279us/step - loss: 137.8572\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 290us/step - loss: 115.7050\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 275us/step - loss: 63.4219\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 280us/step - loss: 58.5245\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 284us/step - loss: 57.5366\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 271us/step - loss: 54.9489\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 278us/step - loss: 65.1079\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 285us/step - loss: 62.5840\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 304us/step - loss: 61.1281\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 304us/step - loss: 60.2320\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 537.8008\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337/337 [==============================] - 0s 83us/step - loss: 456.5506\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 81us/step - loss: 359.3975\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 88us/step - loss: 267.5388\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 76us/step - loss: 102.4244\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 74us/step - loss: 89.7809\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 86us/step - loss: 88.8796\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 78us/step - loss: 86.8100\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 79us/step - loss: 144.0027\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 87us/step - loss: 103.0397\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 87us/step - loss: 84.0765\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 83us/step - loss: 82.5733\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 534.7409\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 272us/step - loss: 223.6844\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 274us/step - loss: 160.3695\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 310us/step - loss: 133.9875\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 288us/step - loss: 68.1280\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 277us/step - loss: 63.0523\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 262us/step - loss: 61.5347\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 270us/step - loss: 60.3851\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 299us/step - loss: 74.9612\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 298us/step - loss: 66.1638\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 310us/step - loss: 63.7474\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 327us/step - loss: 62.2263\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 322.4702\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 281us/step - loss: 153.8950\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 282us/step - loss: 126.0016\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 262us/step - loss: 108.2572\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 262us/step - loss: 62.8147\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 266us/step - loss: 57.5288\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 259us/step - loss: 55.1692\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 260us/step - loss: 53.7598\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 258us/step - loss: 70.1172\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 270us/step - loss: 62.8824\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 256us/step - loss: 61.5788\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 258us/step - loss: 60.1560\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 652.8940\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 69us/step - loss: 521.4705\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 72us/step - loss: 405.3017\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 71us/step - loss: 299.4614\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 70us/step - loss: 114.7017\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 77us/step - loss: 100.4268\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 74us/step - loss: 98.7618\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 71us/step - loss: 94.6495\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 70us/step - loss: 155.2988\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 72us/step - loss: 108.5860\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 72us/step - loss: 86.0195\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 71us/step - loss: 84.4598\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 467.3028\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 264us/step - loss: 181.3294\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 262us/step - loss: 129.4016\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 259us/step - loss: 116.5373\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 277us/step - loss: 67.3118\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 264us/step - loss: 65.9892\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 315us/step - loss: 65.1116\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 285us/step - loss: 64.0299\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 277us/step - loss: 79.0247\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 285us/step - loss: 69.3104\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 266us/step - loss: 67.9208\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 269us/step - loss: 66.5576\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 394.7372\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 282us/step - loss: 216.8757\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 273us/step - loss: 151.5485\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 269us/step - loss: 109.6716\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 279us/step - loss: 60.7253\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 252us/step - loss: 60.7144\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 262us/step - loss: 58.5476\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 255us/step - loss: 59.2904\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 251us/step - loss: 66.8643\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 255us/step - loss: 62.8831\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 250us/step - loss: 60.0840\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 238us/step - loss: 59.5810\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 610.6064\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 73us/step - loss: 488.8092\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 74us/step - loss: 379.6754\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 72us/step - loss: 275.7506\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 72us/step - loss: 98.9378\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 73us/step - loss: 82.9595\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 70us/step - loss: 78.9296\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 74us/step - loss: 75.7336\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 66us/step - loss: 123.5899\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 70us/step - loss: 86.0552\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 78us/step - loss: 77.9489\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 73us/step - loss: 76.9038\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 3ms/step - loss: 306.0724\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 249us/step - loss: 129.5433\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 252us/step - loss: 104.6465\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 242us/step - loss: 92.4990\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 235us/step - loss: 62.0375\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 248us/step - loss: 60.4886\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 246us/step - loss: 58.0859\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 240us/step - loss: 56.5459\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 241us/step - loss: 64.0347\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 241us/step - loss: 61.7907\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 241us/step - loss: 60.7782\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 238us/step - loss: 58.9817\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 335.9794\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 272us/step - loss: 151.1466\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 260us/step - loss: 120.7347\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337/337 [==============================] - 0s 264us/step - loss: 99.7971\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 330us/step - loss: 61.9666\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 275us/step - loss: 60.6369\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 266us/step - loss: 57.4482\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 276us/step - loss: 54.8535\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 271us/step - loss: 63.9845\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 263us/step - loss: 62.1325\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 265us/step - loss: 61.7884\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 267us/step - loss: 60.1690\n",
      "Epoch 1/2\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 403.8381\n",
      "Epoch 2/2\n",
      "337/337 [==============================] - 0s 239us/step - loss: 147.8415\n",
      "Epoch 1/2\n",
      "337/337 [==============================] - 0s 293us/step - loss: 69.6544\n",
      "Epoch 2/2\n",
      "337/337 [==============================] - 0s 247us/step - loss: 65.0011\n",
      "Epoch 1/2\n",
      "338/338 [==============================] - 0s 244us/step - loss: 72.1235\n",
      "Epoch 2/2\n",
      "338/338 [==============================] - 0s 240us/step - loss: 68.8895\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 687.9357\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 334us/step - loss: 343.4764\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 337us/step - loss: 206.8838\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 271us/step - loss: 173.4774\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 277us/step - loss: 85.8863\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 272us/step - loss: 77.5534\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 271us/step - loss: 73.0972\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 315us/step - loss: 69.1192\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 320us/step - loss: 78.2465\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 333us/step - loss: 73.6736\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 279us/step - loss: 70.4928\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 290us/step - loss: 68.7364\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 680.8381\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 72us/step - loss: 595.0780\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 74us/step - loss: 526.8475\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 76us/step - loss: 461.5687\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 75us/step - loss: 253.4098\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 82us/step - loss: 204.9774\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 83us/step - loss: 160.3753\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 76us/step - loss: 127.0737\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 70us/step - loss: 227.7441\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 69us/step - loss: 168.3456\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 74us/step - loss: 121.9161\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 77us/step - loss: 93.9426\n",
      "Epoch 1/3\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 384.5931\n",
      "Epoch 2/3\n",
      "337/337 [==============================] - 0s 239us/step - loss: 154.0459\n",
      "Epoch 3/3\n",
      "337/337 [==============================] - 0s 245us/step - loss: 113.0328\n",
      "Epoch 1/3\n",
      "337/337 [==============================] - 0s 244us/step - loss: 64.4643\n",
      "Epoch 2/3\n",
      "337/337 [==============================] - 0s 262us/step - loss: 63.1588\n",
      "Epoch 3/3\n",
      "337/337 [==============================] - 0s 255us/step - loss: 60.8203\n",
      "Epoch 1/3\n",
      "338/338 [==============================] - 0s 243us/step - loss: 69.7908\n",
      "Epoch 2/3\n",
      "338/338 [==============================] - 0s 263us/step - loss: 64.5155\n",
      "Epoch 3/3\n",
      "338/338 [==============================] - 0s 263us/step - loss: 62.5388\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 509.9938\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 356us/step - loss: 256.3560\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 363us/step - loss: 187.5084\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 330us/step - loss: 166.5253\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 376us/step - loss: 85.4287\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 349us/step - loss: 79.4187\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 270us/step - loss: 75.1611\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 283us/step - loss: 71.5985\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 263us/step - loss: 89.9267\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 276us/step - loss: 72.9172\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 270us/step - loss: 71.4095\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 268us/step - loss: 70.1618\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 178.0764\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 264us/step - loss: 121.3319\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 269us/step - loss: 90.4377\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 275us/step - loss: 77.5613\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 270us/step - loss: 58.0863\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 270us/step - loss: 54.1419\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 270us/step - loss: 52.7875\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 273us/step - loss: 51.8102\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 263us/step - loss: 67.5223\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 270us/step - loss: 58.3474\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 274us/step - loss: 56.6371\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 267us/step - loss: 56.1380\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 532.9571\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 0s 268us/step - loss: 121.7593\n",
      "Epoch 1/1\n",
      "338/338 [==============================] - 0s 280us/step - loss: 114.8976\n",
      "Epoch 1/2\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 357.7034\n",
      "Epoch 2/2\n",
      "337/337 [==============================] - 0s 69us/step - loss: 193.6770\n",
      "Epoch 1/2\n",
      "337/337 [==============================] - 0s 66us/step - loss: 75.6723\n",
      "Epoch 2/2\n",
      "337/337 [==============================] - 0s 77us/step - loss: 72.8630\n",
      "Epoch 1/2\n",
      "338/338 [==============================] - 0s 67us/step - loss: 91.8962\n",
      "Epoch 2/2\n",
      "338/338 [==============================] - 0s 72us/step - loss: 74.7095\n",
      "Epoch 1/3\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 610.7758\n",
      "Epoch 2/3\n",
      "337/337 [==============================] - 0s 273us/step - loss: 335.4596\n",
      "Epoch 3/3\n",
      "337/337 [==============================] - 0s 274us/step - loss: 179.5793\n",
      "Epoch 1/3\n",
      "337/337 [==============================] - 0s 276us/step - loss: 86.9139\n",
      "Epoch 2/3\n",
      "337/337 [==============================] - 0s 277us/step - loss: 80.4557\n",
      "Epoch 3/3\n",
      "337/337 [==============================] - 0s 277us/step - loss: 75.1124\n",
      "Epoch 1/3\n",
      "338/338 [==============================] - 0s 266us/step - loss: 92.2508\n",
      "Epoch 2/3\n",
      "338/338 [==============================] - 0s 272us/step - loss: 75.4884\n",
      "Epoch 3/3\n",
      "338/338 [==============================] - 0s 263us/step - loss: 73.3510\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 488.7307\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 141us/step - loss: 271.1489\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 145us/step - loss: 136.3268\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 144us/step - loss: 115.2144\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 141us/step - loss: 66.9922\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 142us/step - loss: 60.7666\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 140us/step - loss: 59.3052\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 144us/step - loss: 58.1982\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338/338 [==============================] - 0s 139us/step - loss: 74.3886\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 146us/step - loss: 67.4480\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 140us/step - loss: 65.5598\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 143us/step - loss: 64.1212\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 334.3169\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 0s 267us/step - loss: 75.9937\n",
      "Epoch 1/1\n",
      "338/338 [==============================] - 0s 307us/step - loss: 86.7220\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 518.5189\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 140us/step - loss: 223.0280\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 148us/step - loss: 149.4459\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 145us/step - loss: 133.5405\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 143us/step - loss: 72.0448\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 139us/step - loss: 66.2930\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 142us/step - loss: 64.5441\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 150us/step - loss: 63.1328\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 146us/step - loss: 79.7728\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 144us/step - loss: 69.2103\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 148us/step - loss: 67.0259\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 141us/step - loss: 65.9578\n",
      "Epoch 1/2\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 413.2017\n",
      "Epoch 2/2\n",
      "337/337 [==============================] - 0s 281us/step - loss: 228.9868\n",
      "Epoch 1/2\n",
      "337/337 [==============================] - 0s 283us/step - loss: 97.7735\n",
      "Epoch 2/2\n",
      "337/337 [==============================] - 0s 276us/step - loss: 80.5642\n",
      "Epoch 1/2\n",
      "338/338 [==============================] - 0s 285us/step - loss: 87.6966\n",
      "Epoch 2/2\n",
      "338/338 [==============================] - 0s 281us/step - loss: 71.5192\n",
      "Epoch 1/3\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 452.5989\n",
      "Epoch 2/3\n",
      "337/337 [==============================] - 0s 99us/step - loss: 354.0458\n",
      "Epoch 3/3\n",
      "337/337 [==============================] - 0s 91us/step - loss: 261.6179\n",
      "Epoch 1/3\n",
      "337/337 [==============================] - 0s 77us/step - loss: 95.9829\n",
      "Epoch 2/3\n",
      "337/337 [==============================] - 0s 80us/step - loss: 80.3691\n",
      "Epoch 3/3\n",
      "337/337 [==============================] - 0s 78us/step - loss: 78.4952\n",
      "Epoch 1/3\n",
      "338/338 [==============================] - 0s 72us/step - loss: 124.2344\n",
      "Epoch 2/3\n",
      "338/338 [==============================] - 0s 78us/step - loss: 95.7659\n",
      "Epoch 3/3\n",
      "338/338 [==============================] - 0s 78us/step - loss: 81.9460\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 568.8618\n",
      "Epoch 1/1\n",
      "337/337 [==============================] - 0s 146us/step - loss: 236.1381\n",
      "Epoch 1/1\n",
      "338/338 [==============================] - 0s 141us/step - loss: 209.1083\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 1s 4ms/step - loss: 260.7357\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 310us/step - loss: 138.1363\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 323us/step - loss: 103.3476\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 301us/step - loss: 91.3917\n",
      "Epoch 1/4\n",
      "337/337 [==============================] - 0s 257us/step - loss: 63.3243\n",
      "Epoch 2/4\n",
      "337/337 [==============================] - 0s 256us/step - loss: 61.1788\n",
      "Epoch 3/4\n",
      "337/337 [==============================] - 0s 248us/step - loss: 59.5699\n",
      "Epoch 4/4\n",
      "337/337 [==============================] - 0s 303us/step - loss: 59.3615\n",
      "Epoch 1/4\n",
      "338/338 [==============================] - 0s 289us/step - loss: 66.7669\n",
      "Epoch 2/4\n",
      "338/338 [==============================] - 0s 298us/step - loss: 62.7629\n",
      "Epoch 3/4\n",
      "338/338 [==============================] - 0s 334us/step - loss: 61.7197\n",
      "Epoch 4/4\n",
      "338/338 [==============================] - 0s 309us/step - loss: 59.9915\n",
      "{'batch_size': 0, 'dense_1': 10.0, 'dense_2': 6.0, 'epochs': 3, 'num_layers': 1, 'optim': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0:02:32.629263'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import hyperopt.pyll.stochastic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import KFold,ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "t0= time.clock()\n",
    "\n",
    "boston = load_boston()\n",
    "y=boston.target\n",
    "X=boston.data\n",
    "\n",
    "\n",
    "space = {\n",
    "    'dense_1': hp.quniform('dense_1', 5,10,1),\n",
    "    'optim': hp.choice('optim', ['adam', 'rmsprop']),\n",
    "    'batch_size': hp.choice('batch_size', [8,16,32]),\n",
    "    'epochs': hp.choice('epochs', [1,2,3,4]),\n",
    "    \n",
    "    'num_layers' :hp.choice('num_layers',\n",
    "    [\n",
    "                    {'layers':'one',\n",
    "                    \n",
    "                    },\n",
    "        \n",
    "                    {'layers':'two',\n",
    "                      \n",
    "                        'dense_2': hp.quniform('dense_2', 5,10,1)\n",
    "                                \n",
    "                    }\n",
    "\n",
    "    ])\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "kf=KFold(n_splits=3,random_state=4521)\n",
    "\n",
    "def build_model (params):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(params['dense_1']), input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    \n",
    "    if params['num_layers'] == 'two':\n",
    "        model.add(Dense(int(params['dense_2']), kernel_initializer='normal', activation='relu'))\n",
    "      \n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=params['optim'])\n",
    "    return (model)\n",
    "\n",
    "    \n",
    "def obj_function (params):\n",
    "    model=build_model(params)\n",
    "    \n",
    "    hold_actual=np.zeros((X.shape[0],1))\n",
    "    hold_preds=np.zeros((X.shape[0],1))\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train,y_train,batch_size=params['batch_size'], epochs=params['epochs'])\n",
    "        pred=model.predict(X_test)\n",
    "        hold_actual[test_index,0]=y_test.ravel()\n",
    "        hold_preds[test_index,0]=pred.ravel()\n",
    "\n",
    "    return(mean_squared_error(hold_actual,hold_preds))\n",
    "    \n",
    "trials = Trials()\n",
    "best = fmin(obj_function, space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "print (best)\n",
    "\n",
    "t1 = time.clock() - t0\n",
    "\n",
    "str(datetime.timedelta(seconds=t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.trials[4]['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>RAY</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jma/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/jma/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "Process STDOUT and STDERR is being redirected to /tmp/raylogs/.\n",
      "Waiting for redis server at 127.0.0.1:48470 to respond...\n",
      "Waiting for redis server at 127.0.0.1:63739 to respond...\n",
      "Starting local scheduler with the following resources: {'CPU': 8, 'GPU': 1}.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8895/notebooks/ray_ui39983.ipynb?token=0e683471f48dabc4ccd3e699b683099b0ec02c832bc89908\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tPENDING\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam_2018-07-19_15-45-23em6ooaka -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 1/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "RUNNING trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tRUNNING\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam_2018-07-19_15-45-28byf39h8b -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_2_batch_size=16_dense_1=7.0_epochs=1_num_layers={'lay_optim=rmspr:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_2_batch_size=16_dense_1=7.0_epochs=1_num_layers={'lay_optim=rmspr_2018-07-19_15-45-32ozc2xukd -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_3_batch_size=32_dense_1=9.0_epochs=2_num_layers={'lay_optim=adam_2018-07-19_15-45-33wuvaeax8 -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_4_batch_size=8_dense_1=7.0_epochs=2_num_layers={'den_optim=adam_2018-07-19_15-45-36klg56xzy -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_5_batch_size=16_dense_1=8.0_epochs=3_num_layers={'den_optim=rmspr:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_2_batch_size=16_dense_1=7.0_epochs=1_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=25649], 1 s, 1 ts, 252 loss\n",
      " - exp_3_batch_size=32_dense_1=9.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=25696], 2 s, 1 ts, 392 loss\n",
      " - exp_4_batch_size=8_dense_1=7.0_epochs=2_num_layers={'den_optim=adam:\tTERMINATED [pid=25734], 2 s, 1 ts, 169 loss\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_5_batch_size=16_dense_1=8.0_epochs=3_num_layers={'den_optim=rmspr_2018-07-19_15-45-39vnf1oakg -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_6_batch_size=32_dense_1=9.0_epochs=2_num_layers={'den_optim=rmspr_2018-07-19_15-45-41at2k0qih -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_7_batch_size=16_dense_1=6.0_epochs=4_num_layers={'den_optim=adam_2018-07-19_15-45-43rj0bjep_ -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_8_batch_size=16_dense_1=6.0_epochs=4_num_layers={'den_optim=rmspr:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_2_batch_size=16_dense_1=7.0_epochs=1_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=25649], 1 s, 1 ts, 252 loss\n",
      " - exp_3_batch_size=32_dense_1=9.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=25696], 2 s, 1 ts, 392 loss\n",
      " - exp_4_batch_size=8_dense_1=7.0_epochs=2_num_layers={'den_optim=adam:\tTERMINATED [pid=25734], 2 s, 1 ts, 169 loss\n",
      " - exp_5_batch_size=16_dense_1=8.0_epochs=3_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25772], 2 s, 1 ts, 215 loss\n",
      " - exp_6_batch_size=32_dense_1=9.0_epochs=2_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25810], 1 s, 1 ts, 365 loss\n",
      " - exp_7_batch_size=16_dense_1=6.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=25848], 2 s, 1 ts, 281 loss\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_8_batch_size=16_dense_1=6.0_epochs=4_num_layers={'den_optim=rmspr_2018-07-19_15-45-46saxrs35n -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_9_batch_size=16_dense_1=6.0_epochs=2_num_layers={'lay_optim=rmspr_2018-07-19_15-45-4945j03jq1 -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_2_batch_size=16_dense_1=7.0_epochs=1_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=25649], 1 s, 1 ts, 252 loss\n",
      " - exp_3_batch_size=32_dense_1=9.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=25696], 2 s, 1 ts, 392 loss\n",
      " - exp_4_batch_size=8_dense_1=7.0_epochs=2_num_layers={'den_optim=adam:\tTERMINATED [pid=25734], 2 s, 1 ts, 169 loss\n",
      " - exp_5_batch_size=16_dense_1=8.0_epochs=3_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25772], 2 s, 1 ts, 215 loss\n",
      " - exp_6_batch_size=32_dense_1=9.0_epochs=2_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25810], 1 s, 1 ts, 365 loss\n",
      " - exp_7_batch_size=16_dense_1=6.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=25848], 2 s, 1 ts, 281 loss\n",
      " - exp_8_batch_size=16_dense_1=6.0_epochs=4_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25886], 2 s, 1 ts, 86.3 loss\n",
      " - exp_9_batch_size=16_dense_1=6.0_epochs=2_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=25924], 2 s, 1 ts, 197 loss\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam_2018-07-19_15-45-511a3ghcvq -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_11_batch_size=8_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr_2018-07-19_15-45-54umzhwu36 -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_12_batch_size=8_dense_1=5.0_epochs=4_num_layers={'den_optim=adam:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25966], 2 s, 1 ts, 258 loss\n",
      " - exp_11_batch_size=8_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26004], 2 s, 1 ts, 88.5 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_2_batch_size=16_dense_1=7.0_epochs=1_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=25649], 1 s, 1 ts, 252 loss\n",
      " - exp_3_batch_size=32_dense_1=9.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=25696], 2 s, 1 ts, 392 loss\n",
      " - exp_4_batch_size=8_dense_1=7.0_epochs=2_num_layers={'den_optim=adam:\tTERMINATED [pid=25734], 2 s, 1 ts, 169 loss\n",
      " - exp_5_batch_size=16_dense_1=8.0_epochs=3_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25772], 2 s, 1 ts, 215 loss\n",
      " - exp_6_batch_size=32_dense_1=9.0_epochs=2_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25810], 1 s, 1 ts, 365 loss\n",
      " - exp_7_batch_size=16_dense_1=6.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=25848], 2 s, 1 ts, 281 loss\n",
      " - exp_8_batch_size=16_dense_1=6.0_epochs=4_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25886], 2 s, 1 ts, 86.3 loss\n",
      " - exp_9_batch_size=16_dense_1=6.0_epochs=2_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=25924], 2 s, 1 ts, 197 loss\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_12_batch_size=8_dense_1=5.0_epochs=4_num_layers={'den_optim=adam_2018-07-19_15-45-57t0c5kla3 -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_13_batch_size=16_dense_1=8.0_epochs=3_num_layers={'lay_optim=rmspr_2018-07-19_15-46-00mq12w_8m -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_14_batch_size=32_dense_1=5.0_epochs=1_num_layers={'den_optim=adam:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25966], 2 s, 1 ts, 258 loss\n",
      " - exp_11_batch_size=8_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26004], 2 s, 1 ts, 88.5 loss\n",
      " - exp_12_batch_size=8_dense_1=5.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26042], 2 s, 1 ts, 97.3 loss\n",
      " - exp_13_batch_size=16_dense_1=8.0_epochs=3_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26080], 2 s, 1 ts, 171 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_2_batch_size=16_dense_1=7.0_epochs=1_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=25649], 1 s, 1 ts, 252 loss\n",
      " - exp_3_batch_size=32_dense_1=9.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=25696], 2 s, 1 ts, 392 loss\n",
      " - exp_4_batch_size=8_dense_1=7.0_epochs=2_num_layers={'den_optim=adam:\tTERMINATED [pid=25734], 2 s, 1 ts, 169 loss\n",
      " - exp_5_batch_size=16_dense_1=8.0_epochs=3_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25772], 2 s, 1 ts, 215 loss\n",
      " - exp_6_batch_size=32_dense_1=9.0_epochs=2_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25810], 1 s, 1 ts, 365 loss\n",
      " - exp_7_batch_size=16_dense_1=6.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=25848], 2 s, 1 ts, 281 loss\n",
      " - exp_8_batch_size=16_dense_1=6.0_epochs=4_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25886], 2 s, 1 ts, 86.3 loss\n",
      " - exp_9_batch_size=16_dense_1=6.0_epochs=2_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=25924], 2 s, 1 ts, 197 loss\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_14_batch_size=32_dense_1=5.0_epochs=1_num_layers={'den_optim=adam_2018-07-19_15-46-024otly4b2 -> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_15_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr_2018-07-19_15-46-04mlo8qqve -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_16_batch_size=16_dense_1=5.0_epochs=3_num_layers={'den_optim=adam_2018-07-19_15-46-07_3wkozw1 -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_17_batch_size=16_dense_1=7.0_epochs=2_num_layers={'lay_optim=adam:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25966], 2 s, 1 ts, 258 loss\n",
      " - exp_11_batch_size=8_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26004], 2 s, 1 ts, 88.5 loss\n",
      " - exp_12_batch_size=8_dense_1=5.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26042], 2 s, 1 ts, 97.3 loss\n",
      " - exp_13_batch_size=16_dense_1=8.0_epochs=3_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26080], 2 s, 1 ts, 171 loss\n",
      " - exp_14_batch_size=32_dense_1=5.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=26118], 1 s, 1 ts, 383 loss\n",
      " - exp_15_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26158], 2 s, 1 ts, 75.4 loss\n",
      " - exp_16_batch_size=16_dense_1=5.0_epochs=3_num_layers={'den_optim=adam:\tTERMINATED [pid=26196], 2 s, 1 ts, 155 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_2_batch_size=16_dense_1=7.0_epochs=1_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=25649], 1 s, 1 ts, 252 loss\n",
      " - exp_3_batch_size=32_dense_1=9.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=25696], 2 s, 1 ts, 392 loss\n",
      " - exp_4_batch_size=8_dense_1=7.0_epochs=2_num_layers={'den_optim=adam:\tTERMINATED [pid=25734], 2 s, 1 ts, 169 loss\n",
      " - exp_5_batch_size=16_dense_1=8.0_epochs=3_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25772], 2 s, 1 ts, 215 loss\n",
      " - exp_6_batch_size=32_dense_1=9.0_epochs=2_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25810], 1 s, 1 ts, 365 loss\n",
      " - exp_7_batch_size=16_dense_1=6.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=25848], 2 s, 1 ts, 281 loss\n",
      " - exp_8_batch_size=16_dense_1=6.0_epochs=4_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25886], 2 s, 1 ts, 86.3 loss\n",
      " - exp_9_batch_size=16_dense_1=6.0_epochs=2_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=25924], 2 s, 1 ts, 197 loss\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_17_batch_size=16_dense_1=7.0_epochs=2_num_layers={'lay_optim=adam_2018-07-19_15-46-09g7lw43te -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_18_batch_size=8_dense_1=8.0_epochs=4_num_layers={'den_optim=adam_2018-07-19_15-46-12osh_c_ru -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_19_batch_size=8_dense_1=8.0_epochs=3_num_layers={'lay_optim=adam:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25966], 2 s, 1 ts, 258 loss\n",
      " - exp_11_batch_size=8_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26004], 2 s, 1 ts, 88.5 loss\n",
      " - exp_12_batch_size=8_dense_1=5.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26042], 2 s, 1 ts, 97.3 loss\n",
      " - exp_13_batch_size=16_dense_1=8.0_epochs=3_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26080], 2 s, 1 ts, 171 loss\n",
      " - exp_14_batch_size=32_dense_1=5.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=26118], 1 s, 1 ts, 383 loss\n",
      " - exp_15_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26158], 2 s, 1 ts, 75.4 loss\n",
      " - exp_16_batch_size=16_dense_1=5.0_epochs=3_num_layers={'den_optim=adam:\tTERMINATED [pid=26196], 2 s, 1 ts, 155 loss\n",
      " - exp_17_batch_size=16_dense_1=7.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=26234], 2 s, 1 ts, 269 loss\n",
      " - exp_18_batch_size=8_dense_1=8.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26272], 2 s, 1 ts, 108 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_2_batch_size=16_dense_1=7.0_epochs=1_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=25649], 1 s, 1 ts, 252 loss\n",
      " - exp_3_batch_size=32_dense_1=9.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=25696], 2 s, 1 ts, 392 loss\n",
      " - exp_4_batch_size=8_dense_1=7.0_epochs=2_num_layers={'den_optim=adam:\tTERMINATED [pid=25734], 2 s, 1 ts, 169 loss\n",
      " - exp_5_batch_size=16_dense_1=8.0_epochs=3_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25772], 2 s, 1 ts, 215 loss\n",
      " - exp_6_batch_size=32_dense_1=9.0_epochs=2_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25810], 1 s, 1 ts, 365 loss\n",
      " - exp_7_batch_size=16_dense_1=6.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=25848], 2 s, 1 ts, 281 loss\n",
      " - exp_8_batch_size=16_dense_1=6.0_epochs=4_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25886], 2 s, 1 ts, 86.3 loss\n",
      " - exp_9_batch_size=16_dense_1=6.0_epochs=2_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=25924], 2 s, 1 ts, 197 loss\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_19_batch_size=8_dense_1=8.0_epochs=3_num_layers={'lay_optim=adam_2018-07-19_15-46-15gz7zcvr2 -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_20_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr_2018-07-19_15-46-18qo4fivkk -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_21_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25966], 2 s, 1 ts, 258 loss\n",
      " - exp_11_batch_size=8_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26004], 2 s, 1 ts, 88.5 loss\n",
      " - exp_12_batch_size=8_dense_1=5.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26042], 2 s, 1 ts, 97.3 loss\n",
      " - exp_13_batch_size=16_dense_1=8.0_epochs=3_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26080], 2 s, 1 ts, 171 loss\n",
      " - exp_14_batch_size=32_dense_1=5.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=26118], 1 s, 1 ts, 383 loss\n",
      " - exp_15_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26158], 2 s, 1 ts, 75.4 loss\n",
      " - exp_16_batch_size=16_dense_1=5.0_epochs=3_num_layers={'den_optim=adam:\tTERMINATED [pid=26196], 2 s, 1 ts, 155 loss\n",
      " - exp_17_batch_size=16_dense_1=7.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=26234], 2 s, 1 ts, 269 loss\n",
      " - exp_18_batch_size=8_dense_1=8.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26272], 2 s, 1 ts, 108 loss\n",
      " - exp_19_batch_size=8_dense_1=8.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=26310], 2 s, 1 ts, 94.4 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_20_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26348], 2 s, 1 ts, 70 loss\n",
      " - exp_2_batch_size=16_dense_1=7.0_epochs=1_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=25649], 1 s, 1 ts, 252 loss\n",
      " - exp_3_batch_size=32_dense_1=9.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=25696], 2 s, 1 ts, 392 loss\n",
      " - exp_4_batch_size=8_dense_1=7.0_epochs=2_num_layers={'den_optim=adam:\tTERMINATED [pid=25734], 2 s, 1 ts, 169 loss\n",
      " - exp_5_batch_size=16_dense_1=8.0_epochs=3_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25772], 2 s, 1 ts, 215 loss\n",
      " - exp_6_batch_size=32_dense_1=9.0_epochs=2_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25810], 1 s, 1 ts, 365 loss\n",
      " - exp_7_batch_size=16_dense_1=6.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=25848], 2 s, 1 ts, 281 loss\n",
      "  ... 2 more not shown\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_21_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr_2018-07-19_15-46-206iv19tk_ -> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_22_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr_2018-07-19_15-46-23dmrj2jco -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_23_batch_size=32_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25966], 2 s, 1 ts, 258 loss\n",
      " - exp_11_batch_size=8_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26004], 2 s, 1 ts, 88.5 loss\n",
      " - exp_12_batch_size=8_dense_1=5.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26042], 2 s, 1 ts, 97.3 loss\n",
      " - exp_13_batch_size=16_dense_1=8.0_epochs=3_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26080], 2 s, 1 ts, 171 loss\n",
      " - exp_14_batch_size=32_dense_1=5.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=26118], 1 s, 1 ts, 383 loss\n",
      " - exp_15_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26158], 2 s, 1 ts, 75.4 loss\n",
      " - exp_16_batch_size=16_dense_1=5.0_epochs=3_num_layers={'den_optim=adam:\tTERMINATED [pid=26196], 2 s, 1 ts, 155 loss\n",
      " - exp_17_batch_size=16_dense_1=7.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=26234], 2 s, 1 ts, 269 loss\n",
      " - exp_18_batch_size=8_dense_1=8.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26272], 2 s, 1 ts, 108 loss\n",
      " - exp_19_batch_size=8_dense_1=8.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=26310], 2 s, 1 ts, 94.4 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_20_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26348], 2 s, 1 ts, 70 loss\n",
      " - exp_21_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26388], 2 s, 1 ts, 116 loss\n",
      " - exp_22_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26426], 2 s, 1 ts, 108 loss\n",
      " - exp_2_batch_size=16_dense_1=7.0_epochs=1_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=25649], 1 s, 1 ts, 252 loss\n",
      " - exp_3_batch_size=32_dense_1=9.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=25696], 2 s, 1 ts, 392 loss\n",
      " - exp_4_batch_size=8_dense_1=7.0_epochs=2_num_layers={'den_optim=adam:\tTERMINATED [pid=25734], 2 s, 1 ts, 169 loss\n",
      " - exp_5_batch_size=16_dense_1=8.0_epochs=3_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25772], 2 s, 1 ts, 215 loss\n",
      "  ... 4 more not shown\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_23_batch_size=32_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr_2018-07-19_15-46-26gwbs_2bu -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_24_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr_2018-07-19_15-46-292mphgggc -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_25_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25966], 2 s, 1 ts, 258 loss\n",
      " - exp_11_batch_size=8_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26004], 2 s, 1 ts, 88.5 loss\n",
      " - exp_12_batch_size=8_dense_1=5.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26042], 2 s, 1 ts, 97.3 loss\n",
      " - exp_13_batch_size=16_dense_1=8.0_epochs=3_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26080], 2 s, 1 ts, 171 loss\n",
      " - exp_14_batch_size=32_dense_1=5.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=26118], 1 s, 1 ts, 383 loss\n",
      " - exp_15_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26158], 2 s, 1 ts, 75.4 loss\n",
      " - exp_16_batch_size=16_dense_1=5.0_epochs=3_num_layers={'den_optim=adam:\tTERMINATED [pid=26196], 2 s, 1 ts, 155 loss\n",
      " - exp_17_batch_size=16_dense_1=7.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=26234], 2 s, 1 ts, 269 loss\n",
      " - exp_18_batch_size=8_dense_1=8.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26272], 2 s, 1 ts, 108 loss\n",
      " - exp_19_batch_size=8_dense_1=8.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=26310], 2 s, 1 ts, 94.4 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_20_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26348], 2 s, 1 ts, 70 loss\n",
      " - exp_21_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26388], 2 s, 1 ts, 116 loss\n",
      " - exp_22_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26426], 2 s, 1 ts, 108 loss\n",
      " - exp_23_batch_size=32_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26464], 2 s, 1 ts, 140 loss\n",
      " - exp_24_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26502], 2 s, 1 ts, 90.3 loss\n",
      " - exp_2_batch_size=16_dense_1=7.0_epochs=1_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=25649], 1 s, 1 ts, 252 loss\n",
      " - exp_3_batch_size=32_dense_1=9.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=25696], 2 s, 1 ts, 392 loss\n",
      "  ... 6 more not shown\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_25_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr_2018-07-19_15-46-31o2gkmyiv -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_26_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr_2018-07-19_15-46-344lu3gaaf -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_27_batch_size=32_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25966], 2 s, 1 ts, 258 loss\n",
      " - exp_11_batch_size=8_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26004], 2 s, 1 ts, 88.5 loss\n",
      " - exp_12_batch_size=8_dense_1=5.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26042], 2 s, 1 ts, 97.3 loss\n",
      " - exp_13_batch_size=16_dense_1=8.0_epochs=3_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26080], 2 s, 1 ts, 171 loss\n",
      " - exp_14_batch_size=32_dense_1=5.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=26118], 1 s, 1 ts, 383 loss\n",
      " - exp_15_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26158], 2 s, 1 ts, 75.4 loss\n",
      " - exp_16_batch_size=16_dense_1=5.0_epochs=3_num_layers={'den_optim=adam:\tTERMINATED [pid=26196], 2 s, 1 ts, 155 loss\n",
      " - exp_17_batch_size=16_dense_1=7.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=26234], 2 s, 1 ts, 269 loss\n",
      " - exp_18_batch_size=8_dense_1=8.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26272], 2 s, 1 ts, 108 loss\n",
      " - exp_19_batch_size=8_dense_1=8.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=26310], 2 s, 1 ts, 94.4 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_20_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26348], 2 s, 1 ts, 70 loss\n",
      " - exp_21_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26388], 2 s, 1 ts, 116 loss\n",
      " - exp_22_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26426], 2 s, 1 ts, 108 loss\n",
      " - exp_23_batch_size=32_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26464], 2 s, 1 ts, 140 loss\n",
      " - exp_24_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26502], 2 s, 1 ts, 90.3 loss\n",
      " - exp_25_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26541], 2 s, 1 ts, 171 loss\n",
      " - exp_26_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26581], 2 s, 1 ts, 107 loss\n",
      "  ... 8 more not shown\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_27_batch_size=32_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr_2018-07-19_15-46-37f52odblx -> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_28_batch_size=16_dense_1=10.0_epochs=1_num_layers={'lay_optim=rmspr_2018-07-19_15-46-409lg5xvnz -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_29_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr_2018-07-19_15-46-41lc4z0jhp -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_30_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25966], 2 s, 1 ts, 258 loss\n",
      " - exp_11_batch_size=8_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26004], 2 s, 1 ts, 88.5 loss\n",
      " - exp_12_batch_size=8_dense_1=5.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26042], 2 s, 1 ts, 97.3 loss\n",
      " - exp_13_batch_size=16_dense_1=8.0_epochs=3_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26080], 2 s, 1 ts, 171 loss\n",
      " - exp_14_batch_size=32_dense_1=5.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=26118], 1 s, 1 ts, 383 loss\n",
      " - exp_15_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26158], 2 s, 1 ts, 75.4 loss\n",
      " - exp_16_batch_size=16_dense_1=5.0_epochs=3_num_layers={'den_optim=adam:\tTERMINATED [pid=26196], 2 s, 1 ts, 155 loss\n",
      " - exp_17_batch_size=16_dense_1=7.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=26234], 2 s, 1 ts, 269 loss\n",
      " - exp_18_batch_size=8_dense_1=8.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26272], 2 s, 1 ts, 108 loss\n",
      " - exp_19_batch_size=8_dense_1=8.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=26310], 2 s, 1 ts, 94.4 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_20_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26348], 2 s, 1 ts, 70 loss\n",
      " - exp_21_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26388], 2 s, 1 ts, 116 loss\n",
      " - exp_22_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26426], 2 s, 1 ts, 108 loss\n",
      " - exp_23_batch_size=32_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26464], 2 s, 1 ts, 140 loss\n",
      " - exp_24_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26502], 2 s, 1 ts, 90.3 loss\n",
      " - exp_25_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26541], 2 s, 1 ts, 171 loss\n",
      " - exp_26_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26581], 2 s, 1 ts, 107 loss\n",
      "  ... 11 more not shown\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_30_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr_2018-07-19_15-46-441i8uh3a9 -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_31_batch_size=16_dense_1=9.0_epochs=1_num_layers={'lay_optim=rmspr_2018-07-19_15-46-4764ghkokn -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_32_batch_size=32_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr_2018-07-19_15-46-49wuqso03h -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_33_batch_size=16_dense_1=9.0_epochs=3_num_layers={'lay_optim=rmspr:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25966], 2 s, 1 ts, 258 loss\n",
      " - exp_11_batch_size=8_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26004], 2 s, 1 ts, 88.5 loss\n",
      " - exp_12_batch_size=8_dense_1=5.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26042], 2 s, 1 ts, 97.3 loss\n",
      " - exp_13_batch_size=16_dense_1=8.0_epochs=3_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26080], 2 s, 1 ts, 171 loss\n",
      " - exp_14_batch_size=32_dense_1=5.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=26118], 1 s, 1 ts, 383 loss\n",
      " - exp_15_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26158], 2 s, 1 ts, 75.4 loss\n",
      " - exp_16_batch_size=16_dense_1=5.0_epochs=3_num_layers={'den_optim=adam:\tTERMINATED [pid=26196], 2 s, 1 ts, 155 loss\n",
      " - exp_17_batch_size=16_dense_1=7.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=26234], 2 s, 1 ts, 269 loss\n",
      " - exp_18_batch_size=8_dense_1=8.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26272], 2 s, 1 ts, 108 loss\n",
      " - exp_19_batch_size=8_dense_1=8.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=26310], 2 s, 1 ts, 94.4 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_20_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26348], 2 s, 1 ts, 70 loss\n",
      " - exp_21_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26388], 2 s, 1 ts, 116 loss\n",
      " - exp_22_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26426], 2 s, 1 ts, 108 loss\n",
      " - exp_23_batch_size=32_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26464], 2 s, 1 ts, 140 loss\n",
      " - exp_24_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26502], 2 s, 1 ts, 90.3 loss\n",
      " - exp_25_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26541], 2 s, 1 ts, 171 loss\n",
      " - exp_26_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26581], 2 s, 1 ts, 107 loss\n",
      "  ... 14 more not shown\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_33_batch_size=16_dense_1=9.0_epochs=3_num_layers={'lay_optim=rmspr_2018-07-19_15-46-51ji45vb1t -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_34_batch_size=16_dense_1=8.0_epochs=2_num_layers={'lay_optim=rmspr_2018-07-19_15-46-548_p0ot3r -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_35_batch_size=32_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25966], 2 s, 1 ts, 258 loss\n",
      " - exp_11_batch_size=8_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26004], 2 s, 1 ts, 88.5 loss\n",
      " - exp_12_batch_size=8_dense_1=5.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26042], 2 s, 1 ts, 97.3 loss\n",
      " - exp_13_batch_size=16_dense_1=8.0_epochs=3_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26080], 2 s, 1 ts, 171 loss\n",
      " - exp_14_batch_size=32_dense_1=5.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=26118], 1 s, 1 ts, 383 loss\n",
      " - exp_15_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26158], 2 s, 1 ts, 75.4 loss\n",
      " - exp_16_batch_size=16_dense_1=5.0_epochs=3_num_layers={'den_optim=adam:\tTERMINATED [pid=26196], 2 s, 1 ts, 155 loss\n",
      " - exp_17_batch_size=16_dense_1=7.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=26234], 2 s, 1 ts, 269 loss\n",
      " - exp_18_batch_size=8_dense_1=8.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26272], 2 s, 1 ts, 108 loss\n",
      " - exp_19_batch_size=8_dense_1=8.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=26310], 2 s, 1 ts, 94.4 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_20_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26348], 2 s, 1 ts, 70 loss\n",
      " - exp_21_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26388], 2 s, 1 ts, 116 loss\n",
      " - exp_22_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26426], 2 s, 1 ts, 108 loss\n",
      " - exp_23_batch_size=32_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26464], 2 s, 1 ts, 140 loss\n",
      " - exp_24_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26502], 2 s, 1 ts, 90.3 loss\n",
      " - exp_25_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26541], 2 s, 1 ts, 171 loss\n",
      " - exp_26_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26581], 2 s, 1 ts, 107 loss\n",
      "  ... 16 more not shown\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_35_batch_size=32_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr_2018-07-19_15-46-575qw5yz0v -> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_36_batch_size=16_dense_1=10.0_epochs=1_num_layers={'lay_optim=rmspr_2018-07-19_15-46-59vwrti41s -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_37_batch_size=8_dense_1=7.0_epochs=2_num_layers={'lay_optim=rmspr_2018-07-19_15-47-01i7yhut9e -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_38_batch_size=16_dense_1=8.0_epochs=4_num_layers={'lay_optim=rmspr:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25966], 2 s, 1 ts, 258 loss\n",
      " - exp_11_batch_size=8_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26004], 2 s, 1 ts, 88.5 loss\n",
      " - exp_12_batch_size=8_dense_1=5.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26042], 2 s, 1 ts, 97.3 loss\n",
      " - exp_13_batch_size=16_dense_1=8.0_epochs=3_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26080], 2 s, 1 ts, 171 loss\n",
      " - exp_14_batch_size=32_dense_1=5.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=26118], 1 s, 1 ts, 383 loss\n",
      " - exp_15_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26158], 2 s, 1 ts, 75.4 loss\n",
      " - exp_16_batch_size=16_dense_1=5.0_epochs=3_num_layers={'den_optim=adam:\tTERMINATED [pid=26196], 2 s, 1 ts, 155 loss\n",
      " - exp_17_batch_size=16_dense_1=7.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=26234], 2 s, 1 ts, 269 loss\n",
      " - exp_18_batch_size=8_dense_1=8.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26272], 2 s, 1 ts, 108 loss\n",
      " - exp_19_batch_size=8_dense_1=8.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=26310], 2 s, 1 ts, 94.4 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_20_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26348], 2 s, 1 ts, 70 loss\n",
      " - exp_21_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26388], 2 s, 1 ts, 116 loss\n",
      " - exp_22_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26426], 2 s, 1 ts, 108 loss\n",
      " - exp_23_batch_size=32_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26464], 2 s, 1 ts, 140 loss\n",
      " - exp_24_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26502], 2 s, 1 ts, 90.3 loss\n",
      " - exp_25_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26541], 2 s, 1 ts, 171 loss\n",
      " - exp_26_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26581], 2 s, 1 ts, 107 loss\n",
      "  ... 19 more not shown\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_38_batch_size=16_dense_1=8.0_epochs=4_num_layers={'lay_optim=rmspr_2018-07-19_15-47-04zisq0vqc -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_39_batch_size=16_dense_1=10.0_epochs=3_num_layers={'lay_optim=rmspr_2018-07-19_15-47-07vd87nq0q -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_40_batch_size=32_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25966], 2 s, 1 ts, 258 loss\n",
      " - exp_11_batch_size=8_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26004], 2 s, 1 ts, 88.5 loss\n",
      " - exp_12_batch_size=8_dense_1=5.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26042], 2 s, 1 ts, 97.3 loss\n",
      " - exp_13_batch_size=16_dense_1=8.0_epochs=3_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26080], 2 s, 1 ts, 171 loss\n",
      " - exp_14_batch_size=32_dense_1=5.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=26118], 1 s, 1 ts, 383 loss\n",
      " - exp_15_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26158], 2 s, 1 ts, 75.4 loss\n",
      " - exp_16_batch_size=16_dense_1=5.0_epochs=3_num_layers={'den_optim=adam:\tTERMINATED [pid=26196], 2 s, 1 ts, 155 loss\n",
      " - exp_17_batch_size=16_dense_1=7.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=26234], 2 s, 1 ts, 269 loss\n",
      " - exp_18_batch_size=8_dense_1=8.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26272], 2 s, 1 ts, 108 loss\n",
      " - exp_19_batch_size=8_dense_1=8.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=26310], 2 s, 1 ts, 94.4 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_20_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26348], 2 s, 1 ts, 70 loss\n",
      " - exp_21_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26388], 2 s, 1 ts, 116 loss\n",
      " - exp_22_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26426], 2 s, 1 ts, 108 loss\n",
      " - exp_23_batch_size=32_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26464], 2 s, 1 ts, 140 loss\n",
      " - exp_24_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26502], 2 s, 1 ts, 90.3 loss\n",
      " - exp_25_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26541], 2 s, 1 ts, 171 loss\n",
      " - exp_26_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26581], 2 s, 1 ts, 107 loss\n",
      "  ... 21 more not shown\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_40_batch_size=32_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr_2018-07-19_15-47-09a7tjiox1 -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_41_batch_size=16_dense_1=7.0_epochs=2_num_layers={'den_optim=rmspr_2018-07-19_15-47-12hyejx349 -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_42_batch_size=8_dense_1=6.0_epochs=1_num_layers={'lay_optim=rmspr:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25966], 2 s, 1 ts, 258 loss\n",
      " - exp_11_batch_size=8_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26004], 2 s, 1 ts, 88.5 loss\n",
      " - exp_12_batch_size=8_dense_1=5.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26042], 2 s, 1 ts, 97.3 loss\n",
      " - exp_13_batch_size=16_dense_1=8.0_epochs=3_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26080], 2 s, 1 ts, 171 loss\n",
      " - exp_14_batch_size=32_dense_1=5.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=26118], 1 s, 1 ts, 383 loss\n",
      " - exp_15_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26158], 2 s, 1 ts, 75.4 loss\n",
      " - exp_16_batch_size=16_dense_1=5.0_epochs=3_num_layers={'den_optim=adam:\tTERMINATED [pid=26196], 2 s, 1 ts, 155 loss\n",
      " - exp_17_batch_size=16_dense_1=7.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=26234], 2 s, 1 ts, 269 loss\n",
      " - exp_18_batch_size=8_dense_1=8.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26272], 2 s, 1 ts, 108 loss\n",
      " - exp_19_batch_size=8_dense_1=8.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=26310], 2 s, 1 ts, 94.4 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_20_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26348], 2 s, 1 ts, 70 loss\n",
      " - exp_21_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26388], 2 s, 1 ts, 116 loss\n",
      " - exp_22_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26426], 2 s, 1 ts, 108 loss\n",
      " - exp_23_batch_size=32_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26464], 2 s, 1 ts, 140 loss\n",
      " - exp_24_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26502], 2 s, 1 ts, 90.3 loss\n",
      " - exp_25_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26541], 2 s, 1 ts, 171 loss\n",
      " - exp_26_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26581], 2 s, 1 ts, 107 loss\n",
      "  ... 23 more not shown\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_42_batch_size=8_dense_1=6.0_epochs=1_num_layers={'lay_optim=rmspr_2018-07-19_15-47-15s0cw73sa -> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_43_batch_size=16_dense_1=9.0_epochs=4_num_layers={'den_optim=adam_2018-07-19_15-47-17ss4bbqmx -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_44_batch_size=16_dense_1=8.0_epochs=4_num_layers={'lay_optim=rmspr:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25966], 2 s, 1 ts, 258 loss\n",
      " - exp_11_batch_size=8_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26004], 2 s, 1 ts, 88.5 loss\n",
      " - exp_12_batch_size=8_dense_1=5.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26042], 2 s, 1 ts, 97.3 loss\n",
      " - exp_13_batch_size=16_dense_1=8.0_epochs=3_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26080], 2 s, 1 ts, 171 loss\n",
      " - exp_14_batch_size=32_dense_1=5.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=26118], 1 s, 1 ts, 383 loss\n",
      " - exp_15_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26158], 2 s, 1 ts, 75.4 loss\n",
      " - exp_16_batch_size=16_dense_1=5.0_epochs=3_num_layers={'den_optim=adam:\tTERMINATED [pid=26196], 2 s, 1 ts, 155 loss\n",
      " - exp_17_batch_size=16_dense_1=7.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=26234], 2 s, 1 ts, 269 loss\n",
      " - exp_18_batch_size=8_dense_1=8.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26272], 2 s, 1 ts, 108 loss\n",
      " - exp_19_batch_size=8_dense_1=8.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=26310], 2 s, 1 ts, 94.4 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_20_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26348], 2 s, 1 ts, 70 loss\n",
      " - exp_21_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26388], 2 s, 1 ts, 116 loss\n",
      " - exp_22_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26426], 2 s, 1 ts, 108 loss\n",
      " - exp_23_batch_size=32_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26464], 2 s, 1 ts, 140 loss\n",
      " - exp_24_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26502], 2 s, 1 ts, 90.3 loss\n",
      " - exp_25_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26541], 2 s, 1 ts, 171 loss\n",
      " - exp_26_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26581], 2 s, 1 ts, 107 loss\n",
      "  ... 25 more not shown\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_44_batch_size=16_dense_1=8.0_epochs=4_num_layers={'lay_optim=rmspr_2018-07-19_15-47-20t1wr0tjj -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_45_batch_size=32_dense_1=10.0_epochs=3_num_layers={'den_optim=adam_2018-07-19_15-47-236yla8l3u -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_46_batch_size=8_dense_1=6.0_epochs=2_num_layers={'lay_optim=rmspr:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25966], 2 s, 1 ts, 258 loss\n",
      " - exp_11_batch_size=8_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26004], 2 s, 1 ts, 88.5 loss\n",
      " - exp_12_batch_size=8_dense_1=5.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26042], 2 s, 1 ts, 97.3 loss\n",
      " - exp_13_batch_size=16_dense_1=8.0_epochs=3_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26080], 2 s, 1 ts, 171 loss\n",
      " - exp_14_batch_size=32_dense_1=5.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=26118], 1 s, 1 ts, 383 loss\n",
      " - exp_15_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26158], 2 s, 1 ts, 75.4 loss\n",
      " - exp_16_batch_size=16_dense_1=5.0_epochs=3_num_layers={'den_optim=adam:\tTERMINATED [pid=26196], 2 s, 1 ts, 155 loss\n",
      " - exp_17_batch_size=16_dense_1=7.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=26234], 2 s, 1 ts, 269 loss\n",
      " - exp_18_batch_size=8_dense_1=8.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26272], 2 s, 1 ts, 108 loss\n",
      " - exp_19_batch_size=8_dense_1=8.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=26310], 2 s, 1 ts, 94.4 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_20_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26348], 2 s, 1 ts, 70 loss\n",
      " - exp_21_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26388], 2 s, 1 ts, 116 loss\n",
      " - exp_22_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26426], 2 s, 1 ts, 108 loss\n",
      " - exp_23_batch_size=32_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26464], 2 s, 1 ts, 140 loss\n",
      " - exp_24_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26502], 2 s, 1 ts, 90.3 loss\n",
      " - exp_25_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26541], 2 s, 1 ts, 171 loss\n",
      " - exp_26_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26581], 2 s, 1 ts, 107 loss\n",
      "  ... 27 more not shown\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_46_batch_size=8_dense_1=6.0_epochs=2_num_layers={'lay_optim=rmspr_2018-07-19_15-47-26c_rcz2f8 -> \n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_47_batch_size=16_dense_1=10.0_epochs=4_num_layers={'den_optim=adam_2018-07-19_15-47-28wd3vob6j -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "PENDING trials:\n",
      " - exp_48_batch_size=16_dense_1=8.0_epochs=1_num_layers={'lay_optim=rmspr:\tPENDING\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25966], 2 s, 1 ts, 258 loss\n",
      " - exp_11_batch_size=8_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26004], 2 s, 1 ts, 88.5 loss\n",
      " - exp_12_batch_size=8_dense_1=5.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26042], 2 s, 1 ts, 97.3 loss\n",
      " - exp_13_batch_size=16_dense_1=8.0_epochs=3_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26080], 2 s, 1 ts, 171 loss\n",
      " - exp_14_batch_size=32_dense_1=5.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=26118], 1 s, 1 ts, 383 loss\n",
      " - exp_15_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26158], 2 s, 1 ts, 75.4 loss\n",
      " - exp_16_batch_size=16_dense_1=5.0_epochs=3_num_layers={'den_optim=adam:\tTERMINATED [pid=26196], 2 s, 1 ts, 155 loss\n",
      " - exp_17_batch_size=16_dense_1=7.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=26234], 2 s, 1 ts, 269 loss\n",
      " - exp_18_batch_size=8_dense_1=8.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26272], 2 s, 1 ts, 108 loss\n",
      " - exp_19_batch_size=8_dense_1=8.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=26310], 2 s, 1 ts, 94.4 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_20_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26348], 2 s, 1 ts, 70 loss\n",
      " - exp_21_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26388], 2 s, 1 ts, 116 loss\n",
      " - exp_22_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26426], 2 s, 1 ts, 108 loss\n",
      " - exp_23_batch_size=32_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26464], 2 s, 1 ts, 140 loss\n",
      " - exp_24_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26502], 2 s, 1 ts, 90.3 loss\n",
      " - exp_25_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26541], 2 s, 1 ts, 171 loss\n",
      " - exp_26_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26581], 2 s, 1 ts, 107 loss\n",
      "  ... 29 more not shown\n",
      "\n",
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_48_batch_size=16_dense_1=8.0_epochs=1_num_layers={'lay_optim=rmspr_2018-07-19_15-47-31t6fxb0w7 -> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created LogSyncer for /home/jma/ray_results/my_exp/exp_49_batch_size=8_dense_1=7.0_epochs=4_num_layers={'lay_optim=rmspr_2018-07-19_15-47-33dxojcbid -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Result logdir: /home/jma/ray_results/my_exp\n",
      "TERMINATED trials:\n",
      " - exp_0_batch_size=8_dense_1=7.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25521], 2 s, 1 ts, 412 loss\n",
      " - exp_10_batch_size=8_dense_1=6.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=25966], 2 s, 1 ts, 258 loss\n",
      " - exp_11_batch_size=8_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26004], 2 s, 1 ts, 88.5 loss\n",
      " - exp_12_batch_size=8_dense_1=5.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26042], 2 s, 1 ts, 97.3 loss\n",
      " - exp_13_batch_size=16_dense_1=8.0_epochs=3_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26080], 2 s, 1 ts, 171 loss\n",
      " - exp_14_batch_size=32_dense_1=5.0_epochs=1_num_layers={'den_optim=adam:\tTERMINATED [pid=26118], 1 s, 1 ts, 383 loss\n",
      " - exp_15_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26158], 2 s, 1 ts, 75.4 loss\n",
      " - exp_16_batch_size=16_dense_1=5.0_epochs=3_num_layers={'den_optim=adam:\tTERMINATED [pid=26196], 2 s, 1 ts, 155 loss\n",
      " - exp_17_batch_size=16_dense_1=7.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=26234], 2 s, 1 ts, 269 loss\n",
      " - exp_18_batch_size=8_dense_1=8.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=26272], 2 s, 1 ts, 108 loss\n",
      " - exp_19_batch_size=8_dense_1=8.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=26310], 2 s, 1 ts, 94.4 loss\n",
      " - exp_1_batch_size=16_dense_1=7.0_epochs=3_num_layers={'lay_optim=adam:\tTERMINATED [pid=25523], 2 s, 1 ts, 134 loss\n",
      " - exp_20_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26348], 2 s, 1 ts, 70 loss\n",
      " - exp_21_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26388], 2 s, 1 ts, 116 loss\n",
      " - exp_22_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26426], 2 s, 1 ts, 108 loss\n",
      " - exp_23_batch_size=32_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26464], 2 s, 1 ts, 140 loss\n",
      " - exp_24_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26502], 2 s, 1 ts, 90.3 loss\n",
      " - exp_25_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26541], 2 s, 1 ts, 171 loss\n",
      " - exp_26_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26581], 2 s, 1 ts, 107 loss\n",
      " - exp_27_batch_size=32_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26619], 2 s, 1 ts, 148 loss\n",
      " - exp_28_batch_size=16_dense_1=10.0_epochs=1_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26657], 1 s, 1 ts, 205 loss\n",
      " - exp_29_batch_size=16_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26696], 2 s, 1 ts, 87.2 loss\n",
      " - exp_2_batch_size=16_dense_1=7.0_epochs=1_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=25649], 1 s, 1 ts, 252 loss\n",
      " - exp_30_batch_size=16_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26734], 2 s, 1 ts, 99.6 loss\n",
      " - exp_31_batch_size=16_dense_1=9.0_epochs=1_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26772], 1 s, 1 ts, 310 loss\n",
      " - exp_32_batch_size=32_dense_1=10.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26810], 2 s, 1 ts, 82.9 loss\n",
      " - exp_33_batch_size=16_dense_1=9.0_epochs=3_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26848], 2 s, 1 ts, 137 loss\n",
      " - exp_34_batch_size=16_dense_1=8.0_epochs=2_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26886], 2 s, 1 ts, 137 loss\n",
      " - exp_35_batch_size=32_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26924], 2 s, 1 ts, 143 loss\n",
      " - exp_36_batch_size=16_dense_1=10.0_epochs=1_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=26962], 1 s, 1 ts, 282 loss\n",
      " - exp_37_batch_size=8_dense_1=7.0_epochs=2_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=27000], 2 s, 1 ts, 170 loss\n",
      " - exp_38_batch_size=16_dense_1=8.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=27040], 2 s, 1 ts, 90.9 loss\n",
      " - exp_39_batch_size=16_dense_1=10.0_epochs=3_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=27078], 2 s, 1 ts, 124 loss\n",
      " - exp_3_batch_size=32_dense_1=9.0_epochs=2_num_layers={'lay_optim=adam:\tTERMINATED [pid=25696], 2 s, 1 ts, 392 loss\n",
      " - exp_40_batch_size=32_dense_1=9.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=27116], 2 s, 1 ts, 120 loss\n",
      " - exp_41_batch_size=16_dense_1=7.0_epochs=2_num_layers={'den_optim=rmspr:\tTERMINATED [pid=27154], 2 s, 1 ts, 146 loss\n",
      " - exp_42_batch_size=8_dense_1=6.0_epochs=1_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=27192], 2 s, 1 ts, 337 loss\n",
      " - exp_43_batch_size=16_dense_1=9.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=27230], 2 s, 1 ts, 128 loss\n",
      " - exp_44_batch_size=16_dense_1=8.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=27270], 2 s, 1 ts, 109 loss\n",
      " - exp_45_batch_size=32_dense_1=10.0_epochs=3_num_layers={'den_optim=adam:\tTERMINATED [pid=27308], 2 s, 1 ts, 238 loss\n",
      " - exp_46_batch_size=8_dense_1=6.0_epochs=2_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=27346], 2 s, 1 ts, 187 loss\n",
      " - exp_47_batch_size=16_dense_1=10.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=27385], 2 s, 1 ts, 122 loss\n",
      " - exp_48_batch_size=16_dense_1=8.0_epochs=1_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=27423], 1 s, 1 ts, 238 loss\n",
      " - exp_49_batch_size=8_dense_1=7.0_epochs=4_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=27463], 2 s, 1 ts, 85.5 loss\n",
      " - exp_4_batch_size=8_dense_1=7.0_epochs=2_num_layers={'den_optim=adam:\tTERMINATED [pid=25734], 2 s, 1 ts, 169 loss\n",
      " - exp_5_batch_size=16_dense_1=8.0_epochs=3_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25772], 2 s, 1 ts, 215 loss\n",
      " - exp_6_batch_size=32_dense_1=9.0_epochs=2_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25810], 1 s, 1 ts, 365 loss\n",
      " - exp_7_batch_size=16_dense_1=6.0_epochs=4_num_layers={'den_optim=adam:\tTERMINATED [pid=25848], 2 s, 1 ts, 281 loss\n",
      " - exp_8_batch_size=16_dense_1=6.0_epochs=4_num_layers={'den_optim=rmspr:\tTERMINATED [pid=25886], 2 s, 1 ts, 86.3 loss\n",
      " - exp_9_batch_size=16_dense_1=6.0_epochs=2_num_layers={'lay_optim=rmspr:\tTERMINATED [pid=25924], 2 s, 1 ts, 197 loss\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0:00:01.936235'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import ray\n",
    "from ray.tune import run_experiments, register_trainable\n",
    "from ray.tune.hpo_scheduler import HyperOptScheduler\n",
    "\n",
    "import hyperopt.pyll.stochastic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import KFold,ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "t0= time.clock()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "boston = load_boston()\n",
    "y=boston.target\n",
    "X=boston.data\n",
    "\n",
    "\n",
    "space = {\n",
    "    'dense_1': hp.quniform('dense_1', 5,10,1),\n",
    "    'optim': hp.choice('optim', ['adam', 'rmsprop']),\n",
    "    'batch_size': hp.choice('batch_size', [8,16,32]),\n",
    "    'epochs': hp.choice('epochs', [1,2,3,4]),\n",
    "    \n",
    "    'num_layers' :hp.choice('num_layers',\n",
    "    [\n",
    "                    {'layers':'one',\n",
    "                    \n",
    "                    },\n",
    "        \n",
    "                    {'layers':'two',\n",
    "                      \n",
    "                        'dense_2': hp.quniform('dense_2', 5,10,1)\n",
    "                                \n",
    "                    }\n",
    "\n",
    "    ])\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "kf=KFold(n_splits=3,random_state=4521)\n",
    "\n",
    "def build_model (config):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(config['dense_1']), input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    \n",
    "    if config['num_layers'] == 'two':\n",
    "        model.add(Dense(int(config['dense_2']), kernel_initializer='normal', activation='relu'))\n",
    "      \n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=config['optim'])\n",
    "    return (model)\n",
    "\n",
    "    \n",
    "def obj_function (config, reporter):\n",
    "    model=build_model(config)\n",
    "    \n",
    "    hold_actual=np.zeros((X.shape[0],1))\n",
    "    hold_preds=np.zeros((X.shape[0],1))\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train,y_train,batch_size=config['batch_size'], epochs=config['epochs'])\n",
    "        pred=model.predict(X_test)\n",
    "        hold_actual[test_index,0]=y_test.ravel()\n",
    "        hold_preds[test_index,0]=pred.ravel()\n",
    "        \n",
    "    mse=mean_squared_error(hold_actual,hold_preds)\n",
    "    reporter(timesteps_total=1, mean_loss=mse)\n",
    "    \n",
    "\n",
    "    \n",
    "config = {\n",
    "        \"my_exp\": {\n",
    "            \"run\": \"exp\", \n",
    "            \n",
    "            \"stop\": {\n",
    "                \"timesteps_total\": 1\n",
    "            },\n",
    "            \"trial_resources\": { \"cpu\": 8, \"gpu\": 0 },\n",
    "            \"repeat\": 50, #this seems to overide the timesteps_total stop, else it would only run 1 time given above stop setting\n",
    "            \"config\": {\n",
    "                \"space\": space\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "ray.init()\n",
    "register_trainable(\"exp\", obj_function) #registers the above config and the objective function\n",
    "\n",
    "# \"the HyperOptScheduler will take an increasing metric as it's \"reward metric\", \n",
    "#similar to all other schedulers. \n",
    "#However, since we want loss to \"decrease\" = we want neg loss to \"increase\", \n",
    "#we pass \"neg loss\" to the reward metric.\"\n",
    "#so, all schedulers take an increasing metric (meaning they look for higher = better)\n",
    "#negating it allows for preferring a decreasing metric (as a lower loss -> higher neg_loss)\n",
    "\n",
    "hpo_sched = HyperOptScheduler(reward_attr=\"neg_mean_loss\")\n",
    "\n",
    "trials=run_experiments(config, verbose=False, scheduler=hpo_sched)\n",
    "\n",
    "\n",
    "t1 = time.clock() - t0\n",
    "\n",
    "str(datetime.timedelta(seconds=t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials[0].last_result.config['batch_size']\n",
    "trials[0].last_result.mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
